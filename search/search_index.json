{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Machine Learning for Particle Track Reconstruction","text":"<p>Welcome to repository and documentation for ML pipelines and techniques by the ExatrkX Collaboration. Here we present a set of templates, best practices and results gathered from significant trial and error, to speed up the development of others in the domain of machine learning for high energy physics. We focus on applications specific to detector physics, but many tools can be applied to other areas, and these are collected in an application-agnostic way in the Tools section.</p>"},{"location":"#intro","title":"Intro","text":"<p>To start as quickly as possible, clone the repository, Install and follow the steps in Quickstart. This will get you generating toy tracking data and running inference immediately. Many of the choices of structure will be made clear there. If you already have a particle physics problem in mind, you can apply the Template that is most suitable to your use case.</p> <p>Once up and running, you may want to consider more complex ML Models. Many of these are built on other libraries (for example Pytorch Geometric).</p>"},{"location":"#install","title":"Install","text":"<p>Thank god that almost every package used in this library is available on conda and has known compatibility for both CPU and GPU (with CUDA v11.3). Therefore installation should be as simple as:</p> <p>CPU-only installation</p> <pre><code>conda env create -f cpu_environment.yml python=3.9\nconda activate exatrkx-cpu\npip install -e .\n</code></pre> <p>Or, after ensuring your GPU drivers are updated to run CUDA v11.3:</p> <p>GPU installation</p> <pre><code>conda env create -f gpu_environment.yml python=3.9\nconda activate exatrkx-gpu\npip install -e .\n</code></pre> <p>You should be ready for the Quickstart!</p>"},{"location":"#vintage-errors","title":"Vintage Errors","text":"<p>A very possible error will be</p> <pre><code>OSError: libcudart.so.XX.X: cannot open shared object file: No such file or directory\n</code></pre> <p>This indicates a mismatch between CUDA versions. Identify the library that called the error, and ensure there are no versions of this library installed in parallel, e.g. from a previous <code>pip --user</code> install.</p>"},{"location":"models/overview/","title":"Models for Tracking ML","text":"<p>This repository aims to collect modular ML models that can be reused for various HEP applications. However, it is generally directed towards the Exatrkx pipeline for track reconstruction, which handles HEP data as graph-structured. This means that many of the models are related to either:</p> <ol> <li>Constructing graphs (e.g. metric learning, filtering)</li> <li>Learning node or edge feature representations (e.g. edge classification)</li> <li>Segmenting and processing graphs (e.g. segment stitching, splitting)</li> </ol> <p>If you have, for example, image-structured data, the pipeline management tool TrainTrack will still be useful, but many of these models will not. On the other hand, you might consider if your images are representations of some sparse underlying physics whether they could be better-represented as graphs...</p>"},{"location":"models/overview/#including-a-new-model","title":"Including a New Model","text":"<p>As such, new models are welcome to be included. Currently, new models can either be added to the general model collection in <code>Architectures</code> or, if the model is intended to be used in an example application, in that application's pipeline in <code>Pipelines/APPLICATION/LightningModules/</code>. Models are organised by their architecture, which allows much of the common code to be abstracted out. For example, a particular convolution of a GNN can be specified in <code>GNN/Models/my_new_gnn.py</code> which inherits all the training behavior of <code>GNNBase</code> in <code>GNN/gnn_base.py</code>. </p> <p>The models are written in <code>Pytorch Lightning</code>, to avoid as much training boilerplate as possible. To include a new model, or a new base class that doesn't yet exist, it should be implemented in Pytorch Lightning. It usually takes 5-10 minutes to convert vanilla Pytorch code to Pytorch Lightning, see for example this Lightning Tutorial.</p> <p>Once a model is included in a Pipeline, it can included as a stage, as in the TrackML Example.</p>"},{"location":"models/overview/#available-models","title":"Available Models","text":""},{"location":"models/overview/#pre-processing","title":"Pre-Processing","text":"<p>In order to easily consume data for machine learning, it is beneficial to package it into a \"feature store\". This takes the large, possibly messy, possibly text/csv-format, possibly missing dataset and makes it uniform and as light as it can be. The feature store processor here uses the Pytorch Geometric (PyG) <code>data</code> class, for several reasons: a) It is compatible with the PyG <code>dataloader</code> class, which takes it as a simple list and we therefore seriously reduce boilerplate; b) It is a class than can be entirely placed on and off a device with a simple <code>.to(device)</code>; and c) The PyG <code>dataloader</code> automatically batches graph-structured data in the correct way.</p> <p>The <code>FeatureStore</code> class is a <code>LightningDataModule</code> that is compatible with the <code>TrainTrack</code> system - that is, it doesn't do any training or inference, but is instead understood to be a processing stage. It does this by implementing a <code>prepare_data</code> method, and the rest is automatic.</p> <p>In the TrackML example, the most important tasks of <code>FeatureStore</code> processing are to select node (i.e. spacepoint) features and normalise them, and define a truth graph. The rest is housekeeping.</p>"},{"location":"models/overview/#embedding-models-ie-metric-learning","title":"Embedding Models (i.e. Metric Learning)","text":""},{"location":"models/overview/#filtering-models","title":"Filtering Models","text":""},{"location":"models/overview/#graph-neural-networks","title":"Graph Neural Networks","text":""},{"location":"models/overview/#segmentation","title":"Segmentation","text":""},{"location":"models/overview/#top-takeaways","title":"Top Takeaways","text":"<ul> <li>Best model for each use case (by memory, timing, etc.)</li> <li>Best hyperparameter choice</li> </ul>"},{"location":"models/overview/#test-model","title":"Test Model","text":""},{"location":"models/taxonomy/","title":"ML Models Used in Tracking","text":""},{"location":"performance/matching_definitions/","title":"Track Matching Definitions","text":""},{"location":"performance/matching_definitions/#introduction","title":"Introduction","text":"<p>Given that each hit has been assigned (at least) one track label, we would like to quantify how well the labelling was performed. A key set of metrics are: - What proportion of particles were \"reconstructed\" by a track - What proportion of tracks were used to reconstruct particles, which we shall call being \"matched\" to a particle Exactly how we define the goodness of reconstruction and matching is somewhat arbitrary, but there exist some common definitions.</p>"},{"location":"performance/matching_definitions/#styles-of-matching","title":"Styles of Matching","text":"<p>The following infographic explains the three styles of track matching used in this repository (click to zoom in).</p> <p> The three styles of matching - corresponding to <code>matching_style=</code> <code>ATLAS</code>, <code>one_way</code> and <code>two_way</code></p>"},{"location":"performance/metric_definitions/","title":"Metric Definitions","text":""},{"location":"performance/metric_definitions/#metric-learning-metrics","title":"Metric Learning Metrics","text":""},{"location":"performance/metric_definitions/#edge-wise-metrics","title":"Edge-wise Metrics","text":""},{"location":"performance/metric_definitions/#tracking-metrics","title":"Tracking Metrics","text":""},{"location":"performance/metric_definitions/#segmentation-metrics","title":"Segmentation Metrics","text":""},{"location":"performance/overview/","title":"Performance","text":"<p>Here we would like to define the important metrics for each stage, as well as the overall pipeline. For important combinations of models/hyperparameters, the best available benchmarks will also be documented. </p>"},{"location":"performance/truth_definitions/","title":"Truth Definitions","text":""},{"location":"performance/truth_definitions/#trackml-tracking","title":"TrackML Tracking","text":""},{"location":"performance/truth_definitions/#edge-wise-truth","title":"Edge-wise Truth","text":"<p>In training the various stages of the pipeline, two definitions of pair-wise or edge-wise truth are used. The first is the simplest: <code>pid_truth</code>. If two spacepoints share a particle ID (<code>pid</code>), an edge between them is given <code>pid_truth=1</code>. Otherwise <code>pid_truth=0</code>. This also shows up in the library as <code>y_pid</code>, as a toggle that can be turned on/off in GNN training. This truth definition is useful when a graph has already been constructed, as in GNN training. </p> <p>The other definition of truth is <code>modulewise_truth</code>. This is based on the concept of a module-wise truth graph, which will be useful to visualize.</p>"},{"location":"performance/truth_definitions/#truth-graph","title":"Truth Graph","text":"A cartoon of a truth graph, with orange edges and X nodes. Each blue line is a module here (note the dashed layers of the barrel - the layers are made of many small modules) <p>A truth graph is simply some target graph that tries to represent the underlying physics in some way. It must be opinionated, and there is clearly not a single, correct way to define it. That said, for the ITk geometry, we define it by: 1. Order all spacepoints of a track by increasing distance from its creation vertex 2. Connect each pair of spacepoints in this sequence, as an edge in a graph 3. (Module-wise definition) Where two spacepoints are on the same module, do not join them. Instead, form all pair combinations with the preceding and succeeding spacepoints. </p> <p>To phrase this module-wise defintion differently, for the set of spacepoints {x_i} and modules {m_i} in the diagram, we order all spacepoints as</p> <pre><code>[(x_0, m_0), (x_1, m_1), (x_2, m_2), (x_3, m_3), ((x_4, x_5), m_4), (x_6, m_5)]\n</code></pre> <p>Therefore our edge list becomes</p> <pre><code>(x_0, x_1), (x_1, x_2), (x_2, x_3), (x_3, x_4), (x_3, x_5), (x_4, x_6), (x_5, x_6)\n</code></pre> <p>This is achieved quite efficiently in code with: ``` py title=\"get_modulewise_edges()\" ... signal_list = (     signal.groupby(         [\"particle_id\", \"barrel_endcap\", \"layer_disk\", \"eta_module\", \"phi_module\"],         sort=False,     )[\"hit_id\"]     .agg(lambda x: list(x))     .groupby(level=0)     .agg(lambda x: list(x)) )</p> <p>true_edges = [] for row in signal_list.values:     for i, j in zip(row[:-1], row[1:]):         true_edges.extend(list(itertools.product(i, j))) ```</p>"},{"location":"performance/truth_definitions/#modulewise-and-pid-truth","title":"Modulewise and PID Truth","text":"<p>Given these definitions of truth, we can visualize them both on an example graph constructed across two track:</p> Two graphs with different ground truth definitions for the edges. Black edges are true positives, red a false positives, blue are false negatives <p>Note that PID truth is fairly obvious: if an edge connects nodes of the same color (i.e. from the same particle) then it's a true positive, otherwise it's a false positive. In the construction of a graph with this definition of truth, there's no real concept of a \"missing true edge\" - or false negative - since a track of N spacepoints has N choose 2 (O(N^2)) \"true edges\", which is computationally expensive to handle (see Embedding training to understand why). This is why this definition is not used in graph construction. Once constructed, and each existing edge is being classified, it makes more sense to use this definition. </p> <p>The modulewise truth takes the graph on the left, but applies our truth graph to it. Thus, we see that edges that skip across the sequence are treated as false. We also see that we can efficiently define false positives, since each particle of N spacepoints will only have O(N) true edges. The trade-off for this more physically-motivated truth definition is that it's more expensive to calculate the accuracy and loss functions. Instead of simply checking the PID, we have to do a graph intersection, which provides a full list of true/fake positive/negative edges in the prediction graph.</p>"},{"location":"pipelines/overview/","title":"ML Pipelines for Particle Tracking","text":"<p>Follow the tutorial in Quickstart to get up and running quickly with a toy model.</p>"},{"location":"pipelines/overview/#the-front-door","title":"The Front Door","text":"<p>Assuming you've installed the Exatrkx tracking library and tried the Quickstart, then welcome. The aim of this repository is to both be a place to demonstrate our published results and pipelines, and also to show new developments in our work. To that end, various projects (i.e. physics use-cases) are given as templates in the <code>Pipelines</code> directory. To see a frozen reproduction of the pipeline used in our paper, go to <code>Pipelines/TrackML_Example</code>. A relatively frozen example of tracking on the ATLAS ITk detector is given in <code>Pipelines/ITk_Example</code>. Development code is available in <code>Pipelines/Common_Tracking_Example</code>, which is named due to many of the functions working across both detectors. </p> <p>This page gives some outlines and justifications of the choice of the repository's organisation. The Models page defines the models used in track reconstruction. The Tools page describes the various stand-alone functions and concepts that are available in the library and could be useful across models. Performance is where metrics are defined, as well as tracking benchmarks.</p>"},{"location":"pipelines/overview/#how-do-pipelines-work","title":"How do Pipelines work?","text":"<p>The aim of our pipeline structure is to abstract out as much repetitive code as possible. A pipeline is defined by a YAML config file, which only requires three inputs: The location of your model definitions <code>model_library</code>, the location to save/load your artifacts <code>artifact_library</code>, and the stages of the pipeline <code>stage_list</code>. An example stage list is</p> <pre><code>- {set: GNN, name: SliceCheckpointedResAGNN, config: train_gnn.yaml}\n</code></pre> <p>The <code>set</code> key defines the type of ML model (to help avoid naming ambiguity), the <code>name</code> key is the class of the model, and the <code>config</code> key is the file specifying your choice of hyperparameters, directories, callbacks and so on. And that's it!</p> The repository through an application lens, using Pipelines for specific physics goals"},{"location":"pipelines/overview/#why-this-choice-of-abstraction","title":"Why this choice of abstraction?","text":"<p>I found that my two forms of R&amp;D fell into breadth and depth. Much of the time, I would play at length with hyperparameters and model definitions, in which case I want that all to live in one place: The model's config file. Thus the pipeline config can remain untouched if we have one for each choice of (model, config) , or only changed occasionally if we choose to have only one. At other times, development would require a series of models, where successive results depend on hyperparameter choices earlier in the chain. Then I can play with the higher level pipeline config and try difference (model, config) stages, while the whole chain of hyperparameters is committed to each step via a logging platform (Weights &amp; Biases in my case).</p>"},{"location":"pipelines/overview/#pytorch-lightning","title":"Pytorch Lightning","text":"<p>This repository uses Pytorch Lightning, which allows us to encapsulate all training and model logic into a module object. This module is what is being specified by the pipeline config, with <code>name</code>. Combined with callbacks in the model config file, all pipeline logic is contained in each module. A callback object integrates with the module and knows about telemetry and post-processing steps. Rather than a monolithic function that passes data through the pipeline (whereby mistakes could be made), the pipeline asks each model how it should be treated, and then acts with the model's own methods.</p>"},{"location":"pipelines/quickstart/","title":"Quickstart Tutorial","text":""},{"location":"pipelines/quickstart/#1-install","title":"1. Install","text":"<p>See instructions at Install.</p>"},{"location":"pipelines/quickstart/#2-get-dataset","title":"2. Get Dataset","text":"<p>For now, let's keep our data in a directory, with a location saved to <code>/my/data/path</code>: </p> <pre><code>export EXATRKX_DATA=/my/data/path\n</code></pre> <p>(you can hard-code these into your custom configs later).</p> <p>The easiest way to get the TrackML dataset is to use the Kaggle API. Install it with</p> <pre><code>pip install kaggle\n</code></pre> <p>and grab a small toy dataset with</p> <pre><code>kaggle competitions download \\\n    -c trackml-particle-identification \\\n    -f train_sample.zip \\\n    -p $EXATRKX_DATA\n</code></pre>"},{"location":"pipelines/quickstart/#3-running-the-pipeline","title":"3. Running the Pipeline","text":""},{"location":"pipelines/quickstart/#configuration","title":"Configuration","text":"<p>A pipeline runs at three layers of configuration, to allow for as much flexibility as possible. To get running immediately however, you needn't change any of the defaults. From the <code>Pipelines/TrackML_Example/</code> directory, we run</p> <pre><code>traintrack configs/pipeline_quickstart.yaml\n</code></pre> <p>While it's running, get a cup of tea and a Tim-Tam, and let's see what it's doing:</p>"},{"location":"pipelines/quickstart/#default-behaviour","title":"Default behaviour","text":"<p>Our quickstart pipeline is running three stages, with a single configuration for each. You can see in <code>config/pipeline_quickstart.yaml</code> that the three stages are:</p> <ul> <li>A Processing stage with the class <code>FeatureStore</code> and config <code>prepare_small_feature_store.yaml</code>;</li> <li>An Embedding stage with the class <code>LayerlessEmbedding</code> and config <code>train_small_embedding.yaml</code>; and</li> <li>A GNN stage with the class <code>VanillaFilter</code> and config <code>train_small_filter.yaml</code>.</li> </ul> <p>The Processing stage is exactly that: data processing. It is not \"trainable\", and so the pipeline treats it differently than a trainable stage. Under the hood, it is a LightningDataModule, rather than the trainable models, which inherit from LightningModule. In this case, <code>FeatureStore</code> is performing some calculations on the cell information in the detector, and constructing truth graphs that will later be used for training. These calculations are computationally expensive, so it doesn't make sense to calculate them on-the-fly while training. </p> <p>The trainable models Embedding and GNN are learning the non-linear metric of the truth graphs, and pairwise likelihoods of hits sharing a truth graph edge, respectively. The details are not so important at this stage, what matters is that these stages are modular: Each one can be run alone, but by adding a callback to the end, it can prepare the dataset for the next stage. Looking at <code>LightningModules/Embedding/train_small_embedding.yaml</code> you will see that a callback is given as <code>callbacks: EmbeddingInferenceCallback</code>. Any number of callbacks can be added, and they adhere to the Lightning callback system. The one referred to here runs the best version of the trained Embedding model on each directory of the data split (train, val, test) and saves it for the next stage. We could also add a telemetry callback, e.g. the <code>EmbeddingTelemetry</code> callback in <code>LightningModules/Embedding/Models/inference.py</code>. This callback prints a PDF of the transverse momentum vs. the efficiency of the metric learning model, saving it in the <code>output_dir</code>. It \"hooks\" into the testing phase, which is run after every training phase.</p> <p>The default settings of this run pull from the three configuration files given at each stage. You can look at them</p>"},{"location":"tools/data/","title":"Data &amp; Graph Manipulation","text":""},{"location":"tools/data/#data-processing","title":"Data Processing","text":""},{"location":"tools/data/#constructing-truth-graph","title":"Constructing Truth Graph","text":"<p>The purpose of a \"truth graph\" is explained in Truth Definitions. Here we explain how it is constructed.</p> <ol> <li>Begin with a set of hit IDs (HIDs) with associated particle IDs (PIDs). For ITk, this set has duplicated HIDs. This is because two particles may leave a hit in a very coincident space on a module. In that case, we cannot be sure whether the hit is from a single particle, or from many - it is called \"merged\" in the latter case. We only know a hit is \"merged\" from ground truth. Therefore, there is not a unique map from HIDs to PIDs, and we may have many PIDs associated with a single HID. We preserve this many-many map in the input dataframe, in order to construct the truth graph.</li> <li>Calculate the distance of each hit from its creation vertex, and order the entire set by this distance. We now have sequential ordering.</li> <li>Group the entries of this set by PID and module ID. The module ID is whatever combination of identifiers gives a unique module. In the case of ITk, this is <code>[barrel_endcap, layer_disk, eta_module, phi_module]</code>. In the case of TrackML, this is <code>[volume_id, layer_id, module_id]</code>. </li> <li>For each particle, iterate through pairs in the list of hits. Since we group by modules, this list is of the form <code>[[0], [1], [2, 3], [4], ...]</code>, such that where there is more than one entry we take a <code>product</code> of the combinations. E.g. <code>[1] x [2, 3] = [1, 2], [1, 3]</code>.</li> <li>Concatenate all these pairs into an edge list for the full truth graph.</li> <li>Throw away duplicates of HIDs. This is a hack and liable to change. This makes referencing hits very simple, since now there is only one hit ID per node, and only one PID per hit. This contradicts our knowledge of merged hits, but since only ~0.2% of hits are merged, we accept this approximation for now.</li> </ol>"},{"location":"tools/data/#graph-processing","title":"Graph Processing","text":""},{"location":"tools/data/#graph-intersection","title":"Graph Intersection","text":"<p>Consider a \"prediction graph\" and a \"target graph\". How do we compare the two? The simplest way is to treat each edge as a prediction, and see which edges overlap (true positives) and which do not (false positives). This can be done very efficiently with sparse arrays.</p> <ol> <li>Convert predicted <code>e_p</code> and truth <code>e_t</code> edge lists into sparse arrays, with either <code>scipy.sparse</code> or <code>cupy.sparse</code></li> <li>Define an intersection array by</li> </ol> <pre><code>intersection = e_p.multiply(e_t) - ((e_p - e_t) &gt; 0)\n</code></pre> <p>This is simple boolean logic: The first value gives 1 for true positives, and 0 otherwise. The second value gives 1 for false positives, and 0 otherwise. Combining them gives us a sparse array of 1 (for true positives) and -1 (for false positives). Thus when we convert back to a (2, N) edge list, we take <code>intersection &gt; 0</code> to be a true edge, and <code>intersection &lt; 0</code> to be a fake edge.</p>"},{"location":"tools/ml/","title":"Techniques for Training &amp; Inference","text":""},{"location":"tools/overview/","title":"Tools, Techniques and Best Practices","text":"<ul> <li>Explain the rationale and aim</li> <li>Give examples of some tools and applications</li> <li>Radius-graph/knn from FAISS</li> <li>Edge matching</li> <li>Truth graph builder (Pandas building, would be nice to get this on CuPy)</li> <li>Triplet selection (possibly use the PyEmbedding version - benchmark)</li> <li>Triplet/line graph converter</li> <li>Checkpointing</li> <li>Mixed precision</li> <li>Remove duplicate edges</li> <li>Cell feature calculation</li> <li>Training best practices (data balancing, hard negative mining, load balancing)</li> </ul>"}]}