{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Metric Learning in Embedded Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "sys.path.append('..')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from LightningModules.Filter.Models.vanilla_filter import VanillaFilter\n",
    "from LightningModules.Filter.Models.pyramid_filter import PyramidFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pytorch Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example notebook, we will use an approach to ML called Pytorch Lightning. Pytorch is a library like Tensorflow, which is very popular in ML engineering. It's main appeal is foolproof tracking of gradients for backpropagation, and very easy manipulation of tensors on and off GPUs. \n",
    "\n",
    "Pytorch Lightning is an extension of Pytorch that makes some decisions about the best-practices for training. Instead of you writing the training loop yourself, and moving things on and off a GPU, it handles much of this for you. You write all the data loading logic, the loss functions, etc. into a `LightningModule` and then hand this module to a `Trainer`. Together, the module and trainer are the two objects that allow training and inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we start by importing a class that we have written ourselves, in this case a LightningModule that is in charge of loading TrackML (Codalab) data, and training and validating an embedding/metric learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Construct PyLightning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An ML model typically has many knobs to turn, as well as locations of data, some training preferences, and so on. For convenience, let's put all of these parameters into a YAML file and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"example_filter.yaml\") as f:\n",
    "        hparams = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We plug these parameters into a constructor of the `LayerlessEmbedding` Lightning Module. This doesn't **do** anything yet - merely creates the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = VanillaFilter(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! Let's train! We instantiate a `Trainer` class that knows things like which hardware to work with, how long to train for, and a **bunch** of default options that we ignore here. Check out the Trainer class docs in Pytorch Lightning. Suffice it to say that it clears away much repetitive boilerplate in training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='eff',\n",
    "    mode=\"max\",\n",
    "    save_top_k=2,\n",
    "    save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmurnanedaniel\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.9<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">bright-thunder-56</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/murnanedaniel/ITk_1GeV_Filter\" target=\"_blank\">https://wandb.ai/murnanedaniel/ITk_1GeV_Filter</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/murnanedaniel/ITk_1GeV_Filter/runs/3azmqkg2\" target=\"_blank\">https://wandb.ai/murnanedaniel/ITk_1GeV_Filter/runs/3azmqkg2</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20210831_133527-3azmqkg2</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type        | Params\n",
      "---------------------------------------------\n",
      "0 | input_layer  | Linear      | 11.8 K\n",
      "1 | layers       | ModuleList  | 525 K \n",
      "2 | output_layer | Linear      | 513   \n",
      "3 | layernorm    | LayerNorm   | 1.0 K \n",
      "4 | batchnorm    | BatchNorm1d | 1.0 K \n",
      "5 | act          | Tanh        | 0     \n",
      "---------------------------------------------\n",
      "539 K     Trainable params\n",
      "0         Non-trainable params\n",
      "539 K     Total params\n",
      "2.159     Total estimated model params size (MB)\n",
      "/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n",
      "/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95937ca6a87448d8d91000b9d52b885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Starting chunks\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../LightningModules/Filter/filter_base.py:436: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"eff\": torch.tensor(edge_true_positive / edge_true.sum()),\n",
      "../LightningModules/Filter/filter_base.py:437: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"pur\": torch.tensor(edge_true_positive / edge_positive),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "CPU times: user 2min 58s, sys: 49.4 s, total: 3min 47s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logger = WandbLogger(project=\"ITk_1GeV_Filter\", group=\"InitialTest\")\n",
    "trainer = Trainer(gpus=0, max_epochs=1, num_sanity_val_steps=0, logger=logger, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7814c3c3fc4445899049fd0eb74f093e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/global/homes/d/danieltm/ExaTrkX/Tracking-ML-Exa.TrkX/Pipelines/ITk_Example/notebooks/wandb/run-20210831_133527-3azmqkg2/files/ITk_1GeV_Filter/3azmqkg2/checkpoints/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaFilter(checkpoint[\"hyper_parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(resume_from_checkpoint=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c092c45f09641cc951536db2fa872dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExatrkxTest",
   "language": "python",
   "name": "exatrkx-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
