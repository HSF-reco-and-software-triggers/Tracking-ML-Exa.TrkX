{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Graphs for L2IT Collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import logging\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch_geometric as pyg\n",
    "from scipy import sparse as sps\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "# import seaborn as sns\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from notebooks.ITk.utils import *\n",
    "from onetrack import TrackingData\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "TODO:\n",
    "Construction comparison dataset\n",
    "- Download uncorrelated dataset from EOS\n",
    "- Check that these events aren't already in my dataset\n",
    "- Run through the processing\n",
    "- Run through embedding\n",
    "- Run through filter\n",
    "- Check performance is okay... (or not)\n",
    "- Convert to NetworkX\n",
    "- Convert to GraphML\n",
    "\n",
    "Graphs for training\n",
    "- Pull in TF conversion code\n",
    "- Convert filtered graphs to TF records, including edge_index, hit_ids, hit positions, edge features, hit_pids, and signal truth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncorrelated Dataset Processed to GraphML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run preprocessing and compare two events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/global/cfs/cdirs/m3443/data/ITk-upgrade/processed/filter_processed/0GeV_uncorrelated_large/test\"\n",
    "outdir = \"/global/cfs/cdirs/m3443/data/ITk-upgrade/processed/filter_processed/0GeV_uncorrelated_large/graphml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the input directory\n",
    "all_files = os.listdir(input_dir)\n",
    "all_files = [os.path.join(input_dir, file) for file in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "def process_event(event_file):\n",
    "    sample = torch.load(event_file, map_location=\"cpu\")\n",
    "    sample.event_file = os.path.split(sample.event_file)[-1]\n",
    "    if os.path.exists(os.path.join(outdir, f\"{sample.event_file}.graphml\")):\n",
    "        return\n",
    "    sample_nx = pyg.utils.to_networkx(sample, node_attrs=[\"hid\", \"pid\"])\n",
    "    sample_nx.graph[\"event_file\"] = sample.event_file\n",
    "    nx.write_graphml(sample_nx, os.path.join(outdir, f\"{sample.event_file}.graphml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd042dc3e254808bc7b52c29a7de400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = process_map(process_event, all_files, max_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Graphs to TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nodes', 'edges', 'receivers', 'senders', 'globals', 'n_node', 'n_edge')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs.ALL_FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make doublet GraphNtuple\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from graph_nets import graphs\n",
    "\n",
    "graph_types = {\n",
    "    'n_node': tf.int32,\n",
    "    'n_edge': tf.int32,\n",
    "    'nodes': tf.float32,\n",
    "    'edges': tf.float32,\n",
    "    'receivers': tf.int32,\n",
    "    'senders': tf.int32,\n",
    "    'globals': tf.float32,\n",
    "}\n",
    "\n",
    "def parse_tfrec_function(example_proto):\n",
    "    features_description = dict(\n",
    "        [(key+\"_IN\",  tf.io.FixedLenFeature([], tf.string)) for key in graphs.ALL_FIELDS] + \n",
    "        [(key+\"_OUT\", tf.io.FixedLenFeature([], tf.string)) for key in graphs.ALL_FIELDS])\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, features_description)\n",
    "    input_dd = graphs.GraphsTuple(**dict([(key, tf.io.parse_tensor(example[key+\"_IN\"], graph_types[key]))\n",
    "        for key in graphs.ALL_FIELDS]))\n",
    "    out_dd = graphs.GraphsTuple(**dict([(key, tf.io.parse_tensor(example[key+\"_OUT\"], graph_types[key]))\n",
    "        for key in graphs.ALL_FIELDS]))\n",
    "    return input_dd, out_dd\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def serialize_graph(G1, G2):\n",
    "    feature = {}\n",
    "    for key in graphs.ALL_FIELDS:\n",
    "        feature[key+\"_IN\"] = _bytes_feature(tf.io.serialize_tensor(getattr(G1, key)))\n",
    "        feature[key+\"_OUT\"] = _bytes_feature(tf.io.serialize_tensor(getattr(G2, key)))\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "def specs_from_graphs_tuple(\n",
    "    graphs_tuple_sample, with_batch_dim=False,\n",
    "    dynamic_num_graphs=False,\n",
    "    dynamic_num_nodes=True,\n",
    "    dynamic_num_edges=True,\n",
    "    description_fn=tf.TensorSpec,\n",
    "    ):\n",
    "    graphs_tuple_description_fields = {}\n",
    "    edge_dim_fields = [graphs.EDGES, graphs.SENDERS, graphs.RECEIVERS]\n",
    "\n",
    "    for field_name in graphs.ALL_FIELDS:\n",
    "        field_sample = getattr(graphs_tuple_sample, field_name)\n",
    "        if field_sample is None:\n",
    "            raise ValueError(\n",
    "                \"The `GraphsTuple` field `{}` was `None`. All fields of the \"\n",
    "                \"`GraphsTuple` must be specified to create valid signatures that\"\n",
    "                \"work with `tf.function`. This can be achieved with `input_graph = \"\n",
    "                \"utils_tf.set_zero_{{node,edge,global}}_features(input_graph, 0)`\"\n",
    "                \"to replace None's by empty features in your graph. Alternatively\"\n",
    "                \"`None`s can be replaced by empty lists by doing `input_graph = \"\n",
    "                \"input_graph.replace({{nodes,edges,globals}}=[]). To ensure \"\n",
    "                \"correct execution of the program, it is recommended to restore \"\n",
    "                \"the None's once inside of the `tf.function` by doing \"\n",
    "                \"`input_graph = input_graph.replace({{nodes,edges,globals}}=None)\"\n",
    "                \"\".format(field_name))\n",
    "\n",
    "        shape = list(field_sample.shape)\n",
    "        dtype = field_sample.dtype\n",
    "\n",
    "        # If the field is not None but has no field shape (i.e. it is a constant)\n",
    "        # then we consider this to be a replaced `None`.\n",
    "        # If dynamic_num_graphs, then all fields have a None first dimension.\n",
    "        # If dynamic_num_nodes, then the \"nodes\" field needs None first dimension.\n",
    "        # If dynamic_num_edges, then the \"edges\", \"senders\" and \"receivers\" need\n",
    "        # a None first dimension.\n",
    "        if shape:\n",
    "            if with_batch_dim:\n",
    "                shape[1] = None\n",
    "            elif (dynamic_num_graphs \\\n",
    "                or (dynamic_num_nodes \\\n",
    "                    and field_name == graphs.NODES) \\\n",
    "                or (dynamic_num_edges \\\n",
    "                    and field_name in edge_dim_fields)): shape[0] = None\n",
    "\n",
    "        graphs_tuple_description_fields[field_name] = description_fn(\n",
    "            shape=shape, dtype=dtype)\n",
    "\n",
    "    return graphs.GraphsTuple(**graphs_tuple_description_fields)\n",
    "\n",
    "\n",
    "def dtype_shape_from_graphs_tuple(input_graph, with_batch_dim=False,\\\n",
    "                                with_padding=True, debug=False, with_fixed_size=False):\n",
    "    graphs_tuple_dtype = {}\n",
    "    graphs_tuple_shape = {}\n",
    "\n",
    "    edge_dim_fields = [graphs.EDGES, graphs.SENDERS, graphs.RECEIVERS]\n",
    "    for field_name in graphs.ALL_FIELDS:\n",
    "        field_sample = getattr(input_graph, field_name)\n",
    "        shape = list(field_sample.shape)\n",
    "        dtype = field_sample.dtype\n",
    "\n",
    "        if not with_fixed_size and shape and not with_padding:\n",
    "            if with_batch_dim:\n",
    "                shape[1] = None\n",
    "            else:\n",
    "                if field_name == graphs.NODES or field_name in edge_dim_fields:\n",
    "                    shape[0] = None\n",
    "\n",
    "        graphs_tuple_dtype[field_name] = dtype\n",
    "        graphs_tuple_shape[field_name] = tf.TensorShape(shape)\n",
    "        if debug:\n",
    "            print(field_name, shape, dtype)\n",
    "    \n",
    "    return graphs.GraphsTuple(**graphs_tuple_dtype), graphs.GraphsTuple(**graphs_tuple_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Conversion Code\n",
    "\"\"\"\n",
    "base class defines the procedure with that the TFrecord data is produced.\n",
    "\"\"\"\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calc_eta(r, z):\n",
    "    theta = np.arctan2(r, z)\n",
    "    return -1.0 * np.log(np.tan(theta / 2.0))\n",
    "\n",
    "\n",
    "class DoubletsDataset(object):\n",
    "    def __init__(self, num_workers=1, with_padding=False,\n",
    "                n_graphs_per_evt=1, overwrite=False, edge_name='edge_index',\n",
    "                truth_name='y'\n",
    "        ):\n",
    "        self.input_dtype = None\n",
    "        self.input_shape = None\n",
    "        self.target_dtype = None\n",
    "        self.target_shape = None\n",
    "        self.with_padding = False\n",
    "        self.num_workers = num_workers\n",
    "        self.overwrite = overwrite\n",
    "        self.edge_name = edge_name\n",
    "        self.truth_name = truth_name\n",
    "\n",
    "    @staticmethod\n",
    "    def get_edge_features(edge_index, nodes):\n",
    "        edge_delta_eta = calc_eta(nodes[edge_index[0], 0], nodes[edge_index[0], 2]) - calc_eta(nodes[edge_index[1], 0], nodes[edge_index[1], 2])\n",
    "        edge_delta_phi = nodes[edge_index[0], 1] - nodes[edge_index[1], 1]\n",
    "        edge_delta_phi = torch.where(edge_delta_phi > 1, edge_delta_phi - 2, edge_delta_phi)\n",
    "        edge_delta_phi = torch.where(edge_delta_phi < -1, edge_delta_phi + 2, edge_delta_phi)\n",
    "        edge_delta_r = nodes[edge_index[0], 0] - nodes[edge_index[1], 0]\n",
    "        edge_delta_z = nodes[edge_index[0], 2] - nodes[edge_index[1], 2]\n",
    "\n",
    "        edge_features = torch.stack([edge_delta_eta, edge_delta_phi, edge_delta_r, edge_delta_z], dim=1).T\n",
    "        return edge_features\n",
    "\n",
    "    def make_graph(self, event, debug=False):\n",
    "        \"\"\"\n",
    "        Convert the event into a graphs_tuple. \n",
    "        \"\"\"\n",
    "        edge_name = self.edge_name\n",
    "        n_nodes = event['x'].shape[0]\n",
    "        n_edges = event[edge_name].shape[1]\n",
    "        nodes = event['x']\n",
    "        edges = np.zeros((n_edges, 1), dtype=np.float32)\n",
    "        # edges = self.get_edge_features(event[edge_name], nodes) # Returns an array of shape (n_edge_features, n_edges)\n",
    "        senders =  event[edge_name][0, :]\n",
    "        receivers = event[edge_name][1, :]\n",
    "        edge_target = event[self.truth_name].numpy().astype(np.float32)\n",
    "        \n",
    "        input_datadict = {\n",
    "            \"n_node\": n_nodes,\n",
    "            \"n_edge\": n_edges,\n",
    "            \"nodes\": nodes,\n",
    "            \"edges\": edges,\n",
    "            \"senders\": senders,\n",
    "            \"receivers\": receivers,\n",
    "            \"globals\": np.array([n_nodes], dtype=np.float32)\n",
    "        }\n",
    "        n_edges_target = 1\n",
    "        target_datadict = {\n",
    "            \"n_node\": 1,\n",
    "            \"n_edge\": n_edges_target,\n",
    "            \"nodes\": np.zeros((1, 1), dtype=np.float32),\n",
    "            \"edges\": edge_target,\n",
    "            \"senders\": np.zeros((n_edges_target,), dtype=np.int32),\n",
    "            \"receivers\": np.zeros((n_edges_target,), dtype=np.int32),\n",
    "            \"globals\": np.zeros((1,), dtype=np.float32),\n",
    "        }\n",
    "        input_graph = utils_tf.data_dicts_to_graphs_tuple([input_datadict])\n",
    "        target_graph = utils_tf.data_dicts_to_graphs_tuple([target_datadict])\n",
    "        return [(input_graph, target_graph)]        \n",
    "\n",
    "    def _get_signature(self, tensors):\n",
    "        if self.input_dtype and self.target_dtype:\n",
    "            return \n",
    "\n",
    "        ex_input, ex_target = tensors[0]\n",
    "        self.input_dtype, self.input_shape = dtype_shape_from_graphs_tuple(\n",
    "            ex_input, with_padding=self.with_padding)\n",
    "        self.target_dtype, self.target_shape = dtype_shape_from_graphs_tuple(\n",
    "            ex_target, with_padding=self.with_padding)\n",
    "    \n",
    "\n",
    "    def process(self, indir, outdir, max_num_events=None):\n",
    "        files = os.listdir(indir)\n",
    "        if max_num_events:\n",
    "            files = files[:max_num_events]\n",
    "        ievt = 0\n",
    "        now = time.time()\n",
    "        for filename in tqdm(files):\n",
    "            infile = os.path.join(indir, filename)\n",
    "            outname = os.path.join(outdir, filename + \".rec\")\n",
    "            if os.path.exists(outname) and not self.overwrite:\n",
    "                continue\n",
    "            if \"npz\" in infile:\n",
    "                array = np.load(infile)\n",
    "            else:\n",
    "                import torch\n",
    "                array = torch.load(infile, map_location='cpu')\n",
    "            assert (array.hid == torch.arange(len(array.hid))).all()\n",
    "            tensors = self.make_graph(array)\n",
    "            def generator():\n",
    "                for G in tensors:\n",
    "                    yield (G[0], G[1])\n",
    "            self._get_signature(tensors)\n",
    "            dataset = tf.data.Dataset.from_generator(\n",
    "                generator, \n",
    "                output_types=(self.input_dtype, self.target_dtype),\n",
    "                output_shapes=(self.input_shape, self.target_shape),\n",
    "                args=None\n",
    "            )\n",
    "\n",
    "            writer = tf.io.TFRecordWriter(outname)\n",
    "            for data in dataset:\n",
    "                example = serialize_graph(*data)\n",
    "                writer.write(example)\n",
    "            writer.close()\n",
    "            ievt += 1\n",
    "\n",
    "        read_time = time.time() - now\n",
    "        print(\"{} added {:,} events, in {:.1f} mins\".format(self.__class__.__name__,\n",
    "            ievt, read_time/60.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files in folder: /global/cfs/cdirs/m3443/data/ITk-upgrade/processed/filter_processed/0GeV_v3/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes [297481, 3] <dtype: 'float32'>\n",
      "edges [574596, 1] <dtype: 'float32'>\n",
      "receivers [574596] <dtype: 'int32'>\n",
      "senders [574596] <dtype: 'int32'>\n",
      "globals [1, 1] <dtype: 'float32'>\n",
      "n_node [1] <dtype: 'int32'>\n",
      "n_edge [1] <dtype: 'int32'>\n",
      "nodes [1, 1] <dtype: 'float32'>\n",
      "edges [574596] <dtype: 'float32'>\n",
      "receivers [1] <dtype: 'int32'>\n",
      "senders [1] <dtype: 'int32'>\n",
      "globals [1, 1] <dtype: 'float32'>\n",
      "n_node [1] <dtype: 'int32'>\n",
      "n_edge [1] <dtype: 'int32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [21:08<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoubletsDataset added 4,000 events, in 21.1 mins\n",
      "processing files in folder: /global/cfs/cdirs/m3443/data/ITk-upgrade/processed/filter_processed/0GeV_v3/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoubletsDataset added 100 events, in 0.5 mins\n",
      "processing files in folder: /global/cfs/cdirs/m3443/data/ITk-upgrade/processed/filter_processed/0GeV_v3/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoubletsDataset added 50 events, in 0.2 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indir = \"/global/cfs/cdirs/m3443/data/ITk-upgrade/processed/filter_processed/0GeV_v3\"\n",
    "outdir = \"/global/cfs/cdirs/m3443/data/ITk-upgrade/processed/filter_processed/0GeV_v3_TFrecords\"\n",
    "\n",
    "data = DoubletsDataset(num_workers=1, overwrite=True,\n",
    "                        edge_name=\"edge_index\", truth_name=\"y\")\n",
    "\n",
    "datatypes = ['train', 'val', 'test']\n",
    "for datatype in datatypes:\n",
    "    indir = os.path.join(inputdir, datatype)\n",
    "    outname = os.path.join(outdir, datatype)\n",
    "    print(\"processing files in folder: {}\".format(indir))\n",
    "    if not os.path.exists(outname):\n",
    "        os.makedirs(outname, exist_ok=True)\n",
    "    data.process(indir=indir, outdir=outname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "9c9561e687849607a777e3b78997936ee4435483e7c10618ffe52b885841b8c7"
  },
  "kernelspec": {
   "display_name": "ExatrkxGPU",
   "language": "python",
   "name": "exatrkxgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
