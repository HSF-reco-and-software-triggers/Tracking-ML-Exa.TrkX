
Starting sweeps

Running main
Wed Sep 28 10:16:20 2022
Running main
Wed Sep 28 10:16:20 2022
Running main
Wed Sep 28 10:16:20 2022
Running main
Wed Sep 28 10:16:20 2022
Using config file: hetero_reg2_lev2.yaml
Using config file: hetero_reg2_lev2.yaml
Using config file: hetero_reg2_lev2.yaml
Using config file: hetero_reg2_lev2.yaml
Initialising model
Wed Sep 28 10:16:20 2022
Initialising model
Wed Sep 28 10:16:20 2022
Initialising model
Wed Sep 28 10:16:20 2022
Initialising model
Wed Sep 28 10:16:20 2022
../../LightningModules/GNN/Models/submodels/encoders.py:34: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorShape.cpp:2228.)
  self.all_combos = torch.combinations(torch.arange(len(self.hparams["model_ids"])), r=2, with_replacement=True)
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
../../LightningModules/GNN/Models/submodels/encoders.py:34: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorShape.cpp:2228.)
  self.all_combos = torch.combinations(torch.arange(len(self.hparams["model_ids"])), r=2, with_replacement=True)
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
../../LightningModules/GNN/Models/submodels/encoders.py:34: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorShape.cpp:2228.)
  self.all_combos = torch.combinations(torch.arange(len(self.hparams["model_ids"])), r=2, with_replacement=True)
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
../../LightningModules/GNN/Models/submodels/encoders.py:34: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorShape.cpp:2228.)
  self.all_combos = torch.combinations(torch.arange(len(self.hparams["model_ids"])), r=2, with_replacement=True)
wandb: Currently logged in as: pmtuan. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/wandb/run-20220928_101622-2gu1ryed
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-plant-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/pmtuan/ITk_Toy_HeteroGNN
wandb: üöÄ View run at https://wandb.ai/pmtuan/ITk_Toy_HeteroGNN/runs/2gu1ryed
wandb: logging graph, to disable use `wandb.watch(log_graph=False)`
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py:20: LightningDeprecationWarning: The `pl.plugins.training_type.ddp.DDPPlugin` is deprecated in v1.6 and will be removed in v1.8. Use `pl.strategies.ddp.DDPStrategy` instead.
  rank_zero_deprecation(
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

nid002132:75275:75275 [0] NCCL INFO Bootstrap : Using nmn0:10.100.16.120<0>
nid002132:75275:75275 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
nid002132:75275:75275 [0] NCCL INFO NET/IB : No device found.
nid002132:75275:75275 [0] NCCL INFO NET/Socket : Using [0]nmn0:10.100.16.120<0> [1]hsn0:10.249.12.114<0> [2]hsn1:10.249.12.113<0> [3]hsn2:10.249.35.173<0> [4]hsn3:10.249.33.173<0>
nid002132:75275:75275 [0] NCCL INFO Using network Socket
NCCL version 2.10.3+cuda11.3
nid002132:75276:75276 [1] NCCL INFO Bootstrap : Using nmn0:10.100.16.120<0>
nid002132:75277:75277 [2] NCCL INFO Bootstrap : Using nmn0:10.100.16.120<0>
nid002132:75278:75278 [3] NCCL INFO Bootstrap : Using nmn0:10.100.16.120<0>
nid002132:75277:75277 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
nid002132:75276:75276 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
nid002132:75278:75278 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
nid002132:75276:75276 [1] NCCL INFO NET/IB : No device found.
nid002132:75277:75277 [2] NCCL INFO NET/IB : No device found.
nid002132:75278:75278 [3] NCCL INFO NET/IB : No device found.
nid002132:75276:75276 [1] NCCL INFO NET/Socket : Using [0]nmn0:10.100.16.120<0> [1]hsn0:10.249.12.114<0> [2]hsn1:10.249.12.113<0> [3]hsn2:10.249.35.173<0> [4]hsn3:10.249.33.173<0>
nid002132:75276:75276 [1] NCCL INFO Using network Socket
nid002132:75277:75277 [2] NCCL INFO NET/Socket : Using [0]nmn0:10.100.16.120<0> [1]hsn0:10.249.12.114<0> [2]hsn1:10.249.12.113<0> [3]hsn2:10.249.35.173<0> [4]hsn3:10.249.33.173<0>
nid002132:75277:75277 [2] NCCL INFO Using network Socket
nid002132:75278:75278 [3] NCCL INFO NET/Socket : Using [0]nmn0:10.100.16.120<0> [1]hsn0:10.249.12.114<0> [2]hsn1:10.249.12.113<0> [3]hsn2:10.249.35.173<0> [4]hsn3:10.249.33.173<0>
nid002132:75278:75278 [3] NCCL INFO Using network Socket
nid002132:75275:75512 [0] NCCL INFO Channel 00/24 :    0   1   2   3
nid002132:75275:75512 [0] NCCL INFO Channel 01/24 :    0   1   3   2
nid002132:75275:75512 [0] NCCL INFO Channel 02/24 :    0   2   3   1
nid002132:75275:75512 [0] NCCL INFO Channel 03/24 :    0   2   1   3
nid002132:75275:75512 [0] NCCL INFO Channel 04/24 :    0   3   1   2
nid002132:75275:75512 [0] NCCL INFO Channel 05/24 :    0   3   2   1
nid002132:75275:75512 [0] NCCL INFO Channel 06/24 :    0   1   2   3
nid002132:75275:75512 [0] NCCL INFO Channel 07/24 :    0   1   3   2
nid002132:75275:75512 [0] NCCL INFO Channel 08/24 :    0   2   3   1
nid002132:75275:75512 [0] NCCL INFO Channel 09/24 :    0   2   1   3
nid002132:75275:75512 [0] NCCL INFO Channel 10/24 :    0   3   1   2
nid002132:75275:75512 [0] NCCL INFO Channel 11/24 :    0   3   2   1
nid002132:75275:75512 [0] NCCL INFO Channel 12/24 :    0   1   2   3
nid002132:75275:75512 [0] NCCL INFO Channel 13/24 :    0   1   3   2
nid002132:75275:75512 [0] NCCL INFO Channel 14/24 :    0   2   3   1
nid002132:75275:75512 [0] NCCL INFO Channel 15/24 :    0   2   1   3
nid002132:75275:75512 [0] NCCL INFO Channel 16/24 :    0   3   1   2
nid002132:75275:75512 [0] NCCL INFO Channel 17/24 :    0   3   2   1
nid002132:75275:75512 [0] NCCL INFO Channel 18/24 :    0   1   2   3
nid002132:75275:75512 [0] NCCL INFO Channel 19/24 :    0   1   3   2
nid002132:75275:75512 [0] NCCL INFO Channel 20/24 :    0   2   3   1
nid002132:75275:75512 [0] NCCL INFO Channel 21/24 :    0   2   1   3
nid002132:75275:75512 [0] NCCL INFO Channel 22/24 :    0   3   1   2
nid002132:75275:75512 [0] NCCL INFO Channel 23/24 :    0   3   2   1
nid002132:75276:75514 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 3/-1/-1->1->-1 [3] 3/-1/-1->1->-1 [4] -1/-1/-1->1->3 [5] -1/-1/-1->1->3 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 3/-1/-1->1->-1 [11] 3/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 3/-1/-1->1->-1 [15] 3/-1/-1->1->-1 [16] -1/-1/-1->1->3 [17] -1/-1/-1->1->3 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 3/-1/-1->1->-1 [23] 3/-1/-1->1->-1
nid002132:75277:75513 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] -1/-1/-1->2->0 [3] -1/-1/-1->2->0 [4] 0/-1/-1->2->-1 [5] 0/-1/-1->2->-1 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->0 [11] -1/-1/-1->2->0 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] -1/-1/-1->2->0 [15] -1/-1/-1->2->0 [16] 0/-1/-1->2->-1 [17] 0/-1/-1->2->-1 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] -1/-1/-1->2->0 [23] -1/-1/-1->2->0
nid002132:75278:75515 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->1 [3] 0/-1/-1->3->1 [4] 1/-1/-1->3->0 [5] 1/-1/-1->3->0 [6] 2/-1/-1->3->-1 [7] 2/-1/-1->3->-1 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 0/-1/-1->3->1 [11] 0/-1/-1->3->1 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] 0/-1/-1->3->1 [15] 0/-1/-1->3->1 [16] 1/-1/-1->3->0 [17] 1/-1/-1->3->0 [18] 2/-1/-1->3->-1 [19] 2/-1/-1->3->-1 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] 0/-1/-1->3->1 [23] 0/-1/-1->3->1
nid002132:75275:75512 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 2/-1/-1->0->3 [3] 2/-1/-1->0->3 [4] 3/-1/-1->0->2 [5] 3/-1/-1->0->2 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 2/-1/-1->0->3 [11] 2/-1/-1->0->3 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 2/-1/-1->0->3 [15] 2/-1/-1->0->3 [16] 3/-1/-1->0->2 [17] 3/-1/-1->0->2 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 2/-1/-1->0->3 [23] 2/-1/-1->0->3
nid002132:75278:75515 [3] NCCL INFO Channel 00 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 00 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 03 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 04 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 06 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 06 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 00 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 00 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 09 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 10 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 02 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 01 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 12 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 12 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 06 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 06 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 15 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 16 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 08 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 07 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 18 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 18 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 12 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 12 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 21 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 22 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 14 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 13 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 18 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 18 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 20 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 19 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 02 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 01 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 02 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 01 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 04 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 04 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 03 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 03 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 08 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 07 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 07 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 08 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 10 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 10 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 09 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 09 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 14 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 13 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 14 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 13 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 16 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 16 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 15 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 15 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 20 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 19 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 20 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 19 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 22 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 22 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 21 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 21 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 03 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 01 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 04 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 02 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 05 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 05 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 05 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 05 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 09 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 07 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 10 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 08 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 11 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 11 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 11 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 11 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 13 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 15 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 14 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 16 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 17 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 17 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 17 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 17 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 19 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 21 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 20 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 22 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 23 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 23 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 23 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 23 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Connected all rings
nid002132:75278:75515 [3] NCCL INFO Connected all rings
nid002132:75276:75514 [1] NCCL INFO Connected all rings
nid002132:75275:75512 [0] NCCL INFO Connected all rings
nid002132:75276:75514 [1] NCCL INFO Channel 01 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 07 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 02 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 08 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 04 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 09 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 01 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 05 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 13 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 07 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 10 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 19 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 09 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 11 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 08 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 20 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 13 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 14 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 21 : 1[41000] -> 2[82000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 09 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 19 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 16 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 20 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 21 : 2[82000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 17 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 21 : 0[3000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 22 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 23 : 3[c1000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 02 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 04 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 05 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 10 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 04 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 02 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 11 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 05 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 03 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 14 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 05 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 10 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 16 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 11 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 11 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 17 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 03 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 14 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 16 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 22 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 05 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 17 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 15 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 23 : 1[41000] -> 3[c1000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 11 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 17 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 22 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 15 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 23 : 2[82000] -> 0[3000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 23 : 0[3000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 17 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 23 : 3[c1000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 00 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 06 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 08 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 00 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 09 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 01 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 12 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 02 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 06 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 18 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 03 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 07 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 00 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 20 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 08 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 14 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 01 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Channel 21 : 3[c1000] -> 2[82000] via P2P/IPC/read
nid002132:75275:75512 [0] NCCL INFO Channel 15 : 0[3000] -> 3[c1000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 12 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 06 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 13 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 07 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 18 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 09 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 19 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 12 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75277:75513 [2] NCCL INFO Channel 20 : 2[82000] -> 1[41000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 13 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 18 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 19 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75276:75514 [1] NCCL INFO Channel 21 : 1[41000] -> 0[3000] via P2P/IPC/read
nid002132:75278:75515 [3] NCCL INFO Connected all trees
nid002132:75278:75515 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
nid002132:75275:75512 [0] NCCL INFO Connected all trees
nid002132:75275:75512 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
nid002132:75278:75515 [3] NCCL INFO 24 coll channels, 32 p2p channels, 8 p2p channels per peer
nid002132:75277:75513 [2] NCCL INFO Connected all trees
nid002132:75277:75513 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
nid002132:75276:75514 [1] NCCL INFO Connected all trees
nid002132:75276:75514 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
nid002132:75275:75512 [0] NCCL INFO 24 coll channels, 32 p2p channels, 8 p2p channels per peer
nid002132:75277:75513 [2] NCCL INFO 24 coll channels, 32 p2p channels, 8 p2p channels per peer
nid002132:75276:75514 [1] NCCL INFO 24 coll channels, 32 p2p channels, 8 p2p channels per peer
nid002132:75275:75512 [0] NCCL INFO comm 0x15433c003010 rank 0 nranks 4 cudaDev 0 busId 3000 - Init COMPLETE
nid002132:75276:75514 [1] NCCL INFO comm 0x154038003010 rank 1 nranks 4 cudaDev 1 busId 41000 - Init COMPLETE
nid002132:75277:75513 [2] NCCL INFO comm 0x14fc2c003010 rank 2 nranks 4 cudaDev 2 busId 82000 - Init COMPLETE
nid002132:75278:75515 [3] NCCL INFO comm 0x149dcc003010 rank 3 nranks 4 cudaDev 3 busId c1000 - Init COMPLETE
nid002132:75275:75275 [0] NCCL INFO Launch mode Parallel
Restoring states from the checkpoint path at 3280036/hpc_ckpt_1.ckpt
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:342: UserWarning: The dirpath has changed from '/global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/ITk_Toy_HeteroGNN/2n62psrh/checkpoints' to '/global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/ITk_Toy_HeteroGNN/2gu1ryed/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
  warnings.warn(
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:342: UserWarning: The dirpath has changed from '/global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/ITk_Toy_HeteroGNN/2n62psrh/checkpoints' to '/global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/ITk_Toy_HeteroGNN/2gu1ryed/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
  warnings.warn(
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:342: UserWarning: The dirpath has changed from '/global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/ITk_Toy_HeteroGNN/2n62psrh/checkpoints' to '/global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/ITk_Toy_HeteroGNN/2gu1ryed/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
  warnings.warn(
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:342: UserWarning: The dirpath has changed from '/global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/ITk_Toy_HeteroGNN/2n62psrh/checkpoints' to '/global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/ITk_Toy_HeteroGNN/2gu1ryed/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
  warnings.warn(
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name    | Type          | Params
------------------------------------------
0 | encoder | HeteroEncoder | 67.3 K
1 | conv    | ModuleList    | 103 K 
2 | decoder | HomoDecoder   | 20.7 K
------------------------------------------
191 K     Trainable params
0         Non-trainable params
191 K     Total params
0.765     Total estimated model params size (MB)
Restored all states from the checkpoint file at 3280036/hpc_ckpt_1.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 160it [00:00, ?it/s]Training:   0% 0/270 [00:00<00:00, -9586980.57it/s]Epoch 12:   0% 0/270 [00:00<00:00, -1991360.95it/s]/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:135: UserWarning: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or enabling fault-tolerant training: https://pytorch-lightning.readthedocs.io/en/stable/advanced/fault_tolerant_training.html
  rank_zero_warn(
Epoch 12:   0% 1/270 [00:03<-1:59:55, -48.26it/s]  Epoch 12:   0% 1/270 [00:03<-1:59:55, -48.26it/s]Epoch 12:   0% 1/270 [00:03<-1:59:54, -44.55it/s, loss=nan, v_num=ryed]Epoch 12:   1% 2/270 [00:04<-1:59:53, -35.32it/s, loss=nan, v_num=ryed]Epoch 12:   1% 2/270 [00:04<-1:59:53, -35.31it/s, loss=nan, v_num=ryed]Epoch 12:   1% 2/270 [00:04<-1:59:53, -35.31it/s, loss=nan, v_num=ryed]Epoch 12:   1% 3/270 [00:05<-1:59:52, -29.74it/s, loss=nan, v_num=ryed]Epoch 12:   1% 3/270 [00:05<-1:59:52, -29.74it/s, loss=nan, v_num=ryed]Epoch 12:   1% 3/270 [00:05<-1:59:51, -29.19it/s, loss=nan, v_num=ryed]Epoch 12:   1% 4/270 [00:06<-1:59:50, -24.26it/s, loss=nan, v_num=ryed]Epoch 12:   1% 4/270 [00:06<-1:59:50, -24.26it/s, loss=nan, v_num=ryed]Epoch 12:   1% 4/270 [00:06<-1:59:49, -23.34it/s, loss=nan, v_num=ryed]Epoch 12:   2% 5/270 [00:07<-1:59:48, -20.85it/s, loss=nan, v_num=ryed]Epoch 12:   2% 5/270 [00:07<-1:59:48, -20.85it/s, loss=nan, v_num=ryed]Epoch 12:   2% 5/270 [00:07<-1:59:47, -20.03it/s, loss=nan, v_num=ryed]Epoch 12:   2% 6/270 [00:08<-1:59:45, -17.53it/s, loss=nan, v_num=ryed]Epoch 12:   2% 6/270 [00:08<-1:59:45, -17.53it/s, loss=nan, v_num=ryed]Epoch 12:   2% 6/270 [00:08<-1:59:45, -17.53it/s, loss=nan, v_num=ryed]Epoch 12:   3% 7/270 [00:09<-1:59:44, -16.28it/s, loss=nan, v_num=ryed]Epoch 12:   3% 7/270 [00:09<-1:59:44, -16.28it/s, loss=nan, v_num=ryed]Epoch 12:   3% 7/270 [00:09<-1:59:44, -15.89it/s, loss=nan, v_num=ryed]Epoch 12:   3% 8/270 [00:10<-1:59:43, -14.72it/s, loss=nan, v_num=ryed]Epoch 12:   3% 8/270 [00:10<-1:59:43, -14.72it/s, loss=nan, v_num=ryed]Epoch 12:   3% 8/270 [00:10<-1:59:42, -14.04it/s, loss=nan, v_num=ryed]Epoch 12:   3% 9/270 [00:11<-1:59:40, -12.68it/s, loss=nan, v_num=ryed]Epoch 12:   3% 9/270 [00:11<-1:59:40, -12.68it/s, loss=nan, v_num=ryed]Epoch 12:   3% 9/270 [00:11<-1:59:40, -12.68it/s, loss=nan, v_num=ryed]Epoch 12:   4% 10/270 [00:12<-1:59:38, -11.75it/s, loss=nan, v_num=ryed]Epoch 12:   4% 10/270 [00:12<-1:59:38, -11.75it/s, loss=nan, v_num=ryed]Epoch 12:   4% 10/270 [00:12<-1:59:38, -11.75it/s, loss=nan, v_num=ryed]Epoch 12:   4% 11/270 [00:13<-1:59:37, -10.88it/s, loss=nan, v_num=ryed]Epoch 12:   4% 11/270 [00:13<-1:59:37, -10.88it/s, loss=nan, v_num=ryed]Epoch 12:   4% 11/270 [00:13<-1:59:36, -10.65it/s, loss=nan, v_num=ryed]Epoch 12:   4% 12/270 [00:14<-1:59:35, -10.07it/s, loss=nan, v_num=ryed]Epoch 12:   4% 12/270 [00:14<-1:59:35, -10.07it/s, loss=nan, v_num=ryed]Epoch 12:   4% 12/270 [00:15<-1:59:34, -9.79it/s, loss=nan, v_num=ryed] Epoch 12:   5% 13/270 [00:16<-1:59:33, -9.18it/s, loss=nan, v_num=ryed]Epoch 12:   5% 13/270 [00:16<-1:59:33, -9.18it/s, loss=nan, v_num=ryed]Epoch 12:   5% 13/270 [00:16<-1:59:32, -9.15it/s, loss=nan, v_num=ryed]Epoch 12:   5% 14/270 [00:16<-1:59:31, -8.60it/s, loss=nan, v_num=ryed]Epoch 12:   5% 14/270 [00:16<-1:59:31, -8.60it/s, loss=nan, v_num=ryed]Epoch 12:   5% 14/270 [00:16<-1:59:31, -8.59it/s, loss=nan, v_num=ryed]Epoch 12:   6% 15/270 [00:17<-1:59:29, -8.16it/s, loss=nan, v_num=ryed]Epoch 12:   6% 15/270 [00:17<-1:59:29, -8.16it/s, loss=nan, v_num=ryed]Epoch 12:   6% 15/270 [00:18<-1:59:29, -8.06it/s, loss=nan, v_num=ryed]Epoch 12:   6% 16/270 [00:18<-1:59:27, -7.63it/s, loss=nan, v_num=ryed]Epoch 12:   6% 16/270 [00:18<-1:59:27, -7.63it/s, loss=nan, v_num=ryed]Epoch 12:   6% 16/270 [00:18<-1:59:27, -7.59it/s, loss=nan, v_num=ryed]Epoch 12:   6% 17/270 [00:19<-1:59:26, -7.27it/s, loss=nan, v_num=ryed]Epoch 12:   6% 17/270 [00:19<-1:59:26, -7.27it/s, loss=nan, v_num=ryed]Epoch 12:   6% 17/270 [00:19<-1:59:25, -7.19it/s, loss=nan, v_num=ryed]Epoch 12:   7% 18/270 [00:20<-1:59:24, -6.83it/s, loss=nan, v_num=ryed]Epoch 12:   7% 18/270 [00:20<-1:59:24, -6.83it/s, loss=nan, v_num=ryed]Epoch 12:   7% 18/270 [00:20<-1:59:24, -6.83it/s, loss=nan, v_num=ryed]Epoch 12:   7% 19/270 [00:21<-1:59:22, -6.51it/s, loss=nan, v_num=ryed]Epoch 12:   7% 19/270 [00:21<-1:59:22, -6.51it/s, loss=nan, v_num=ryed]Epoch 12:   7% 19/270 [00:21<-1:59:22, -6.46it/s, loss=nan, v_num=ryed]Epoch 12:   7% 20/270 [00:22<-1:59:20, -6.20it/s, loss=nan, v_num=ryed]Epoch 12:   7% 20/270 [00:22<-1:59:20, -6.20it/s, loss=nan, v_num=ryed]Epoch 12:   7% 20/270 [00:22<-1:59:20, -6.11it/s, loss=nan, v_num=ryed]Epoch 12:   8% 21/270 [00:24<-1:59:17, -5.79it/s, loss=nan, v_num=ryed]Epoch 12:   8% 21/270 [00:24<-1:59:17, -5.79it/s, loss=nan, v_num=ryed]Epoch 12:   8% 21/270 [00:24<-1:59:17, -5.79it/s, loss=nan, v_num=ryed]Epoch 12:   8% 22/270 [00:24<-1:59:16, -5.57it/s, loss=nan, v_num=ryed]Epoch 12:   8% 22/270 [00:24<-1:59:16, -5.57it/s, loss=nan, v_num=ryed]Epoch 12:   8% 22/270 [00:25<-1:59:15, -5.45it/s, loss=nan, v_num=ryed]Epoch 12:   9% 23/270 [00:26<-1:59:14, -5.26it/s, loss=nan, v_num=ryed]Epoch 12:   9% 23/270 [00:26<-1:59:14, -5.26it/s, loss=nan, v_num=ryed]Epoch 12:   9% 23/270 [00:26<-1:59:13, -5.19it/s, loss=nan, v_num=ryed]Epoch 12:   9% 24/270 [00:27<-1:59:11, -4.99it/s, loss=nan, v_num=ryed]Epoch 12:   9% 24/270 [00:27<-1:59:11, -4.99it/s, loss=nan, v_num=ryed]Epoch 12:   9% 24/270 [00:27<-1:59:11, -4.95it/s, loss=nan, v_num=ryed]/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 284216. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 315361. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 294024. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 349455. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 278466. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 343763. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 243841. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 266132. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 358346. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 304527. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 322662. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 273534. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 272533. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 317136. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 281524. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 309792. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 267561. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 315585. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 306995. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 271763. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 359431. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 272214. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 264390. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 305901. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 278374. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Epoch 12:   9% 25/270 [00:28<-1:59:09, -4.78it/s, loss=nan, v_num=ryed]Epoch 12:   9% 25/270 [00:28<-1:59:09, -4.78it/s, loss=nan, v_num=ryed]Epoch 12:   9% 25/270 [00:28<-1:59:09, -4.73it/s, loss=nan, v_num=ryed]Epoch 12:  10% 26/270 [00:29<-1:59:07, -4.55it/s, loss=nan, v_num=ryed]Epoch 12:  10% 26/270 [00:29<-1:59:07, -4.55it/s, loss=nan, v_num=ryed]Epoch 12:  10% 26/270 [00:29<-1:59:06, -4.52it/s, loss=nan, v_num=ryed]Epoch 12:  10% 27/270 [00:30<-1:59:05, -4.35it/s, loss=nan, v_num=ryed]Epoch 12:  10% 27/270 [00:30<-1:59:05, -4.35it/s, loss=nan, v_num=ryed]Epoch 12:  10% 27/270 [00:30<-1:59:04, -4.33it/s, loss=nan, v_num=ryed]Epoch 12:  10% 28/270 [00:31<-1:59:03, -4.21it/s, loss=nan, v_num=ryed]Epoch 12:  10% 28/270 [00:31<-1:59:03, -4.21it/s, loss=nan, v_num=ryed]Epoch 12:  10% 28/270 [00:31<-1:59:03, -4.18it/s, loss=nan, v_num=ryed]Epoch 12:  11% 29/270 [00:32<-1:59:00, -4.01it/s, loss=nan, v_num=ryed]Epoch 12:  11% 29/270 [00:32<-1:59:00, -4.01it/s, loss=nan, v_num=ryed]Epoch 12:  11% 29/270 [00:32<-1:59:00, -4.01it/s, loss=nan, v_num=ryed]Epoch 12:  11% 30/270 [00:33<-1:58:58, -3.87it/s, loss=nan, v_num=ryed]Epoch 12:  11% 30/270 [00:33<-1:58:58, -3.87it/s, loss=nan, v_num=ryed]Epoch 12:  11% 30/270 [00:33<-1:58:58, -3.86it/s, loss=nan, v_num=ryed]Epoch 12:  11% 31/270 [00:34<-1:58:57, -3.74it/s, loss=nan, v_num=ryed]Epoch 12:  11% 31/270 [00:34<-1:58:57, -3.74it/s, loss=nan, v_num=ryed]Epoch 12:  11% 31/270 [00:34<-1:58:56, -3.71it/s, loss=nan, v_num=ryed]Epoch 12:  12% 32/270 [00:35<-1:58:54, -3.60it/s, loss=nan, v_num=ryed]Epoch 12:  12% 32/270 [00:35<-1:58:54, -3.60it/s, loss=nan, v_num=ryed]Epoch 12:  12% 32/270 [00:35<-1:58:54, -3.59it/s, loss=nan, v_num=ryed]Epoch 12:  12% 33/270 [00:36<-1:58:52, -3.47it/s, loss=nan, v_num=ryed]Epoch 12:  12% 33/270 [00:36<-1:58:52, -3.47it/s, loss=nan, v_num=ryed]Epoch 12:  12% 33/270 [00:36<-1:58:52, -3.47it/s, loss=nan, v_num=ryed]Epoch 12:  13% 34/270 [00:37<-1:58:50, -3.36it/s, loss=nan, v_num=ryed]Epoch 12:  13% 34/270 [00:37<-1:58:50, -3.36it/s, loss=nan, v_num=ryed]Epoch 12:  13% 34/270 [00:37<-1:58:50, -3.36it/s, loss=nan, v_num=ryed]Epoch 12:  13% 35/270 [00:38<-1:58:48, -3.25it/s, loss=nan, v_num=ryed]Epoch 12:  13% 35/270 [00:38<-1:58:48, -3.25it/s, loss=nan, v_num=ryed]Epoch 12:  13% 35/270 [00:38<-1:58:48, -3.25it/s, loss=nan, v_num=ryed]Epoch 12:  13% 36/270 [00:39<-1:58:46, -3.15it/s, loss=nan, v_num=ryed]Epoch 12:  13% 36/270 [00:39<-1:58:46, -3.15it/s, loss=nan, v_num=ryed]Epoch 12:  13% 36/270 [00:39<-1:58:46, -3.15it/s, loss=nan, v_num=ryed]Epoch 12:  14% 37/270 [00:40<-1:58:44, -3.05it/s, loss=nan, v_num=ryed]Epoch 12:  14% 37/270 [00:40<-1:58:44, -3.05it/s, loss=nan, v_num=ryed]Epoch 12:  14% 37/270 [00:40<-1:58:44, -3.05it/s, loss=nan, v_num=ryed]Epoch 12:  14% 38/270 [00:41<-1:58:42, -2.96it/s, loss=nan, v_num=ryed]Epoch 12:  14% 38/270 [00:41<-1:58:42, -2.96it/s, loss=nan, v_num=ryed]Epoch 12:  14% 38/270 [00:41<-1:58:42, -2.96it/s, loss=nan, v_num=ryed]Epoch 12:  14% 39/270 [00:42<-1:58:39, -2.85it/s, loss=nan, v_num=ryed]Epoch 12:  14% 39/270 [00:42<-1:58:39, -2.85it/s, loss=nan, v_num=ryed]Epoch 12:  14% 39/270 [00:42<-1:58:39, -2.85it/s, loss=nan, v_num=ryed]Epoch 12:  15% 40/270 [00:43<-1:58:37, -2.77it/s, loss=nan, v_num=ryed]Epoch 12:  15% 40/270 [00:43<-1:58:37, -2.77it/s, loss=nan, v_num=ryed]Epoch 12:  15% 40/270 [00:43<-1:58:37, -2.77it/s, loss=nan, v_num=ryed]Epoch 12:  15% 41/270 [00:44<-1:58:36, -2.70it/s, loss=nan, v_num=ryed]Epoch 12:  15% 41/270 [00:44<-1:58:36, -2.70it/s, loss=nan, v_num=ryed]Epoch 12:  15% 41/270 [00:44<-1:58:35, -2.68it/s, loss=nan, v_num=ryed]Epoch 12:  16% 42/270 [00:45<-1:58:33, -2.60it/s, loss=nan, v_num=ryed]Epoch 12:  16% 42/270 [00:45<-1:58:33, -2.60it/s, loss=nan, v_num=ryed]Epoch 12:  16% 42/270 [00:45<-1:58:33, -2.60it/s, loss=nan, v_num=ryed]Epoch 12:  16% 43/270 [00:46<-1:58:30, -2.52it/s, loss=nan, v_num=ryed]Epoch 12:  16% 43/270 [00:46<-1:58:30, -2.52it/s, loss=nan, v_num=ryed]Epoch 12:  16% 43/270 [00:46<-1:58:30, -2.51it/s, loss=nan, v_num=ryed]Epoch 12:  16% 44/270 [00:47<-1:58:28, -2.43it/s, loss=nan, v_num=ryed]Epoch 12:  16% 44/270 [00:47<-1:58:28, -2.43it/s, loss=nan, v_num=ryed]Epoch 12:  16% 44/270 [00:47<-1:58:28, -2.43it/s, loss=nan, v_num=ryed]Epoch 12:  17% 45/270 [00:48<-1:58:26, -2.37it/s, loss=nan, v_num=ryed]Epoch 12:  17% 45/270 [00:48<-1:58:26, -2.37it/s, loss=nan, v_num=ryed]Epoch 12:  17% 45/270 [00:48<-1:58:25, -2.36it/s, loss=nan, v_num=ryed]Epoch 12:  17% 46/270 [00:49<-1:58:23, -2.30it/s, loss=nan, v_num=ryed]Epoch 12:  17% 46/270 [00:49<-1:58:23, -2.30it/s, loss=nan, v_num=ryed]Epoch 12:  17% 46/270 [00:49<-1:58:23, -2.30it/s, loss=nan, v_num=ryed]Epoch 12:  17% 47/270 [00:50<-1:58:21, -2.25it/s, loss=nan, v_num=ryed]Epoch 12:  17% 47/270 [00:50<-1:58:21, -2.25it/s, loss=nan, v_num=ryed]Epoch 12:  17% 47/270 [00:50<-1:58:21, -2.23it/s, loss=nan, v_num=ryed]Epoch 12:  18% 48/270 [00:51<-1:58:18, -2.17it/s, loss=nan, v_num=ryed]Epoch 12:  18% 48/270 [00:51<-1:58:18, -2.17it/s, loss=nan, v_num=ryed]Epoch 12:  18% 48/270 [00:51<-1:58:18, -2.17it/s, loss=nan, v_num=ryed]Epoch 12:  18% 49/270 [00:52<-1:58:16, -2.11it/s, loss=nan, v_num=ryed]Epoch 12:  18% 49/270 [00:52<-1:58:16, -2.11it/s, loss=nan, v_num=ryed]Epoch 12:  18% 49/270 [00:52<-1:58:16, -2.11it/s, loss=nan, v_num=ryed]/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 319555. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 329030. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 255243. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 358606. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 322247. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 289481. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 286758. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 327451. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 301963. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 349829. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 314367. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 310426. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 304187. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 392085. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 318226. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 290234. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 330206. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 349072. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 338323. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 293779. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 315446. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 264915. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 344951. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 339564. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 283284. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Epoch 12:  19% 50/270 [00:53<-1:58:14, -2.06it/s, loss=nan, v_num=ryed]Epoch 12:  19% 50/270 [00:53<-1:58:14, -2.06it/s, loss=nan, v_num=ryed]Epoch 12:  19% 50/270 [00:53<-1:58:13, -2.05it/s, loss=nan, v_num=ryed]Epoch 12:  19% 51/270 [00:54<-1:58:11, -2.00it/s, loss=nan, v_num=ryed]Epoch 12:  19% 51/270 [00:54<-1:58:11, -2.00it/s, loss=nan, v_num=ryed]Epoch 12:  19% 51/270 [00:54<-1:58:11, -1.99it/s, loss=nan, v_num=ryed]Epoch 12:  19% 52/270 [00:55<-1:58:09, -1.95it/s, loss=nan, v_num=ryed]Epoch 12:  19% 52/270 [00:55<-1:58:09, -1.95it/s, loss=nan, v_num=ryed]Epoch 12:  19% 52/270 [00:55<-1:58:08, -1.94it/s, loss=nan, v_num=ryed]Epoch 12:  20% 53/270 [00:56<-1:58:06, -1.90it/s, loss=nan, v_num=ryed]Epoch 12:  20% 53/270 [00:56<-1:58:06, -1.90it/s, loss=nan, v_num=ryed]Epoch 12:  20% 53/270 [00:56<-1:58:05, -1.89it/s, loss=nan, v_num=ryed]Epoch 12:  20% 54/270 [00:57<-1:58:03, -1.83it/s, loss=nan, v_num=ryed]Epoch 12:  20% 54/270 [00:57<-1:58:03, -1.83it/s, loss=nan, v_num=ryed]Epoch 12:  20% 54/270 [00:57<-1:58:03, -1.83it/s, loss=nan, v_num=ryed]Epoch 12:  20% 55/270 [00:58<-1:58:00, -1.79it/s, loss=nan, v_num=ryed]Epoch 12:  20% 55/270 [00:58<-1:58:00, -1.79it/s, loss=nan, v_num=ryed]Epoch 12:  20% 55/270 [00:58<-1:58:00, -1.79it/s, loss=nan, v_num=ryed]Epoch 12:  21% 56/270 [00:59<-1:57:58, -1.75it/s, loss=nan, v_num=ryed]Epoch 12:  21% 56/270 [00:59<-1:57:58, -1.75it/s, loss=nan, v_num=ryed]Epoch 12:  21% 56/270 [00:59<-1:57:58, -1.74it/s, loss=nan, v_num=ryed]Epoch 12:  21% 57/270 [01:00<-1:57:55, -1.70it/s, loss=nan, v_num=ryed]Epoch 12:  21% 57/270 [01:00<-1:57:55, -1.70it/s, loss=nan, v_num=ryed]Epoch 12:  21% 57/270 [01:00<-1:57:55, -1.70it/s, loss=nan, v_num=ryed]Epoch 12:  21% 58/270 [01:01<-1:57:52, -1.65it/s, loss=nan, v_num=ryed]Epoch 12:  21% 58/270 [01:01<-1:57:52, -1.65it/s, loss=nan, v_num=ryed]Epoch 12:  21% 58/270 [01:01<-1:57:52, -1.65it/s, loss=nan, v_num=ryed]Epoch 12:  22% 59/270 [01:02<-1:57:50, -1.61it/s, loss=nan, v_num=ryed]Epoch 12:  22% 59/270 [01:02<-1:57:50, -1.61it/s, loss=nan, v_num=ryed]Epoch 12:  22% 59/270 [01:02<-1:57:50, -1.61it/s, loss=nan, v_num=ryed]Epoch 12:  22% 60/270 [01:03<-1:57:47, -1.58it/s, loss=nan, v_num=ryed]Epoch 12:  22% 60/270 [01:03<-1:57:47, -1.58it/s, loss=nan, v_num=ryed]Epoch 12:  22% 60/270 [01:03<-1:57:47, -1.57it/s, loss=nan, v_num=ryed]Epoch 12:  23% 61/270 [01:04<-1:57:44, -1.53it/s, loss=nan, v_num=ryed]Epoch 12:  23% 61/270 [01:04<-1:57:44, -1.53it/s, loss=nan, v_num=ryed]Epoch 12:  23% 61/270 [01:04<-1:57:44, -1.53it/s, loss=nan, v_num=ryed]Epoch 12:  23% 62/270 [01:05<-1:57:41, -1.49it/s, loss=nan, v_num=ryed]Epoch 12:  23% 62/270 [01:05<-1:57:41, -1.49it/s, loss=nan, v_num=ryed]Epoch 12:  23% 62/270 [01:05<-1:57:41, -1.49it/s, loss=nan, v_num=ryed]Epoch 12:  23% 63/270 [01:06<-1:57:39, -1.46it/s, loss=nan, v_num=ryed]Epoch 12:  23% 63/270 [01:06<-1:57:39, -1.46it/s, loss=nan, v_num=ryed]Epoch 12:  23% 63/270 [01:06<-1:57:38, -1.46it/s, loss=nan, v_num=ryed]Epoch 12:  24% 64/270 [01:07<-1:57:36, -1.43it/s, loss=nan, v_num=ryed]Epoch 12:  24% 64/270 [01:07<-1:57:36, -1.43it/s, loss=nan, v_num=ryed]Epoch 12:  24% 64/270 [01:07<-1:57:35, -1.42it/s, loss=nan, v_num=ryed]Epoch 12:  24% 65/270 [01:08<-1:57:33, -1.39it/s, loss=nan, v_num=ryed]Epoch 12:  24% 65/270 [01:08<-1:57:33, -1.39it/s, loss=nan, v_num=ryed]Epoch 12:  24% 65/270 [01:08<-1:57:33, -1.39it/s, loss=nan, v_num=ryed]Epoch 12:  24% 66/270 [01:09<-1:57:30, -1.35it/s, loss=nan, v_num=ryed]Epoch 12:  24% 66/270 [01:09<-1:57:30, -1.35it/s, loss=nan, v_num=ryed]Epoch 12:  24% 66/270 [01:09<-1:57:29, -1.35it/s, loss=nan, v_num=ryed]Epoch 12:  25% 67/270 [01:10<-1:57:26, -1.32it/s, loss=nan, v_num=ryed]Epoch 12:  25% 67/270 [01:10<-1:57:26, -1.32it/s, loss=nan, v_num=ryed]Epoch 12:  25% 67/270 [01:10<-1:57:26, -1.32it/s, loss=nan, v_num=ryed]Epoch 12:  25% 68/270 [01:11<-1:57:24, -1.29it/s, loss=nan, v_num=ryed]Epoch 12:  25% 68/270 [01:11<-1:57:24, -1.29it/s, loss=nan, v_num=ryed]Epoch 12:  25% 68/270 [01:11<-1:57:23, -1.29it/s, loss=nan, v_num=ryed]Epoch 12:  26% 69/270 [01:12<-1:57:21, -1.26it/s, loss=nan, v_num=ryed]Epoch 12:  26% 69/270 [01:12<-1:57:21, -1.26it/s, loss=nan, v_num=ryed]Epoch 12:  26% 69/270 [01:12<-1:57:20, -1.25it/s, loss=nan, v_num=ryed]Epoch 12:  26% 70/270 [01:13<-1:57:18, -1.23it/s, loss=nan, v_num=ryed]Epoch 12:  26% 70/270 [01:13<-1:57:18, -1.23it/s, loss=nan, v_num=ryed]Epoch 12:  26% 70/270 [01:13<-1:57:17, -1.23it/s, loss=nan, v_num=ryed]Epoch 12:  26% 71/270 [01:14<-1:57:14, -1.20it/s, loss=nan, v_num=ryed]Epoch 12:  26% 71/270 [01:14<-1:57:14, -1.20it/s, loss=nan, v_num=ryed]Epoch 12:  26% 71/270 [01:14<-1:57:14, -1.20it/s, loss=nan, v_num=ryed]Epoch 12:  27% 72/270 [01:15<-1:57:12, -1.17it/s, loss=nan, v_num=ryed]Epoch 12:  27% 72/270 [01:15<-1:57:12, -1.17it/s, loss=nan, v_num=ryed]Epoch 12:  27% 72/270 [01:15<-1:57:11, -1.17it/s, loss=nan, v_num=ryed]Epoch 12:  27% 73/270 [01:16<-1:57:08, -1.14it/s, loss=nan, v_num=ryed]Epoch 12:  27% 73/270 [01:16<-1:57:08, -1.14it/s, loss=nan, v_num=ryed]Epoch 12:  27% 73/270 [01:16<-1:57:08, -1.14it/s, loss=nan, v_num=ryed]Epoch 12:  27% 74/270 [01:17<-1:57:05, -1.12it/s, loss=nan, v_num=ryed]Epoch 12:  27% 74/270 [01:17<-1:57:05, -1.12it/s, loss=nan, v_num=ryed]Epoch 12:  27% 74/270 [01:17<-1:57:04, -1.11it/s, loss=nan, v_num=ryed]/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 317734. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 291305. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 290259. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 364032. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 305468. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 277222. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 343153. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 321254. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 325847. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 283281. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 293784. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 342088. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 319529. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 244157. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 306188. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 318306. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 317458. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 285055. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 256640. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 255767. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 325285. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 247106. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 315916. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 295625. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 310572. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Epoch 12:  28% 75/270 [01:18<-1:57:01, -1.09it/s, loss=nan, v_num=ryed]Epoch 12:  28% 75/270 [01:18<-1:57:01, -1.09it/s, loss=nan, v_num=ryed]Epoch 12:  28% 75/270 [01:18<-1:57:01, -1.08it/s, loss=nan, v_num=ryed]Epoch 12:  28% 76/270 [01:19<-1:56:57, -1.06it/s, loss=nan, v_num=ryed]Epoch 12:  28% 76/270 [01:19<-1:56:57, -1.06it/s, loss=nan, v_num=ryed]Epoch 12:  28% 76/270 [01:19<-1:56:57, -1.06it/s, loss=nan, v_num=ryed]Epoch 12:  29% 77/270 [01:20<-1:56:54, -1.04it/s, loss=nan, v_num=ryed]Epoch 12:  29% 77/270 [01:20<-1:56:54, -1.04it/s, loss=nan, v_num=ryed]Epoch 12:  29% 77/270 [01:20<-1:56:54, -1.03it/s, loss=nan, v_num=ryed]Epoch 12:  29% 78/270 [01:21<-1:56:50, -1.01it/s, loss=nan, v_num=ryed]Epoch 12:  29% 78/270 [01:21<-1:56:50, -1.01it/s, loss=nan, v_num=ryed]Epoch 12:  29% 78/270 [01:21<-1:56:50, -1.01it/s, loss=nan, v_num=ryed]Epoch 12:  29% 79/270 [01:22<-1:56:46, -0.98it/s, loss=nan, v_num=ryed]Epoch 12:  29% 79/270 [01:22<-1:56:46, -0.98it/s, loss=nan, v_num=ryed]Epoch 12:  29% 79/270 [01:22<-1:56:46, -0.98it/s, loss=nan, v_num=ryed]Epoch 12:  30% 80/270 [01:23<-1:56:43, -0.96it/s, loss=nan, v_num=ryed]Epoch 12:  30% 80/270 [01:23<-1:56:43, -0.96it/s, loss=nan, v_num=ryed]Epoch 12:  30% 80/270 [01:23<-1:56:43, -0.96it/s, loss=nan, v_num=ryed]Epoch 12:  30% 81/270 [01:24<-1:56:39, -0.94it/s, loss=nan, v_num=ryed]Epoch 12:  30% 81/270 [01:24<-1:56:39, -0.94it/s, loss=nan, v_num=ryed]Epoch 12:  30% 81/270 [01:24<-1:56:39, -0.94it/s, loss=nan, v_num=ryed]Epoch 12:  30% 82/270 [01:24<-1:56:36, -0.92it/s, loss=nan, v_num=ryed]Epoch 12:  30% 82/270 [01:24<-1:56:36, -0.92it/s, loss=nan, v_num=ryed]Epoch 12:  30% 82/270 [01:25<-1:56:35, -0.91it/s, loss=nan, v_num=ryed]Epoch 12:  31% 83/270 [01:26<-1:56:31, -0.89it/s, loss=nan, v_num=ryed]Epoch 12:  31% 83/270 [01:26<-1:56:31, -0.89it/s, loss=nan, v_num=ryed]Epoch 12:  31% 83/270 [01:26<-1:56:31, -0.89it/s, loss=nan, v_num=ryed]Epoch 12:  31% 84/270 [01:27<-1:56:27, -0.87it/s, loss=nan, v_num=ryed]Epoch 12:  31% 84/270 [01:27<-1:56:27, -0.87it/s, loss=nan, v_num=ryed]Epoch 12:  31% 84/270 [01:27<-1:56:27, -0.87it/s, loss=nan, v_num=ryed]Epoch 12:  31% 85/270 [01:28<-1:56:23, -0.85it/s, loss=nan, v_num=ryed]Epoch 12:  31% 85/270 [01:28<-1:56:23, -0.85it/s, loss=nan, v_num=ryed]Epoch 12:  31% 85/270 [01:28<-1:56:23, -0.85it/s, loss=nan, v_num=ryed]Epoch 12:  32% 86/270 [01:29<-1:56:18, -0.83it/s, loss=nan, v_num=ryed]Epoch 12:  32% 86/270 [01:29<-1:56:18, -0.83it/s, loss=nan, v_num=ryed]Epoch 12:  32% 86/270 [01:29<-1:56:18, -0.83it/s, loss=nan, v_num=ryed]Epoch 12:  32% 87/270 [01:30<-1:56:14, -0.81it/s, loss=nan, v_num=ryed]Epoch 12:  32% 87/270 [01:30<-1:56:14, -0.81it/s, loss=nan, v_num=ryed]Epoch 12:  32% 87/270 [01:30<-1:56:14, -0.81it/s, loss=nan, v_num=ryed]Epoch 12:  33% 88/270 [01:31<-1:56:10, -0.79it/s, loss=nan, v_num=ryed]Epoch 12:  33% 88/270 [01:31<-1:56:10, -0.79it/s, loss=nan, v_num=ryed]Epoch 12:  33% 88/270 [01:31<-1:56:10, -0.79it/s, loss=nan, v_num=ryed]Epoch 12:  33% 89/270 [01:32<-1:56:05, -0.77it/s, loss=nan, v_num=ryed]Epoch 12:  33% 89/270 [01:32<-1:56:05, -0.77it/s, loss=nan, v_num=ryed]Epoch 12:  33% 89/270 [01:32<-1:56:05, -0.77it/s, loss=nan, v_num=ryed]Epoch 12:  33% 90/270 [01:33<-1:56:01, -0.75it/s, loss=nan, v_num=ryed]Epoch 12:  33% 90/270 [01:33<-1:56:01, -0.75it/s, loss=nan, v_num=ryed]Epoch 12:  33% 90/270 [01:33<-1:56:01, -0.75it/s, loss=nan, v_num=ryed]Epoch 12:  34% 91/270 [01:34<-1:55:57, -0.73it/s, loss=nan, v_num=ryed]Epoch 12:  34% 91/270 [01:34<-1:55:57, -0.73it/s, loss=nan, v_num=ryed]Epoch 12:  34% 91/270 [01:34<-1:55:56, -0.73it/s, loss=nan, v_num=ryed]
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 348379. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 273553. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 289124. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 319634. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 295889. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 360856. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 262220. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 320118. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 296730. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 261245. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 343391. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 345411. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 285364. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 341160. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 275228. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 277135. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Validation: 0it [00:00, ?it/s][A
Validation:   0% 0/20 [00:00<?, ?it/s][A
Validation DataLoader 0:   0% 0/20 [00:00<?, ?it/s][A
Validation DataLoader 0:   5% 1/20 [00:00<00:06,  3.13it/s][A
Validation DataLoader 0:   5% 1/20 [00:00<00:06,  3.13it/s][AEpoch 12:  34% 92/270 [01:35<-1:55:51, -0.71it/s, loss=nan, v_num=ryed]Epoch 12:  34% 92/270 [01:35<-1:55:51, -0.71it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  10% 2/20 [00:01<00:13,  1.33it/s][A
Validation DataLoader 0:  10% 2/20 [00:01<00:13,  1.33it/s][AEpoch 12:  34% 93/270 [01:36<-1:55:46, -0.70it/s, loss=nan, v_num=ryed]Epoch 12:  34% 93/270 [01:36<-1:55:46, -0.70it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  15% 3/20 [00:02<00:13,  1.26it/s][A
Validation DataLoader 0:  15% 3/20 [00:02<00:13,  1.26it/s][AEpoch 12:  35% 94/270 [01:37<-1:55:41, -0.68it/s, loss=nan, v_num=ryed]Epoch 12:  35% 94/270 [01:37<-1:55:41, -0.68it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  20% 4/20 [00:02<00:12,  1.33it/s][A
Validation DataLoader 0:  20% 4/20 [00:02<00:12,  1.33it/s][AEpoch 12:  35% 95/270 [01:37<-1:55:37, -0.66it/s, loss=nan, v_num=ryed]Epoch 12:  35% 95/270 [01:37<-1:55:37, -0.66it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  25% 5/20 [00:03<00:10,  1.47it/s][A
Validation DataLoader 0:  25% 5/20 [00:03<00:10,  1.47it/s][AEpoch 12:  36% 96/270 [01:38<-1:55:33, -0.65it/s, loss=nan, v_num=ryed]Epoch 12:  36% 96/270 [01:38<-1:55:33, -0.65it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  30% 6/20 [00:03<00:08,  1.66it/s][A
Validation DataLoader 0:  30% 6/20 [00:03<00:08,  1.66it/s][AEpoch 12:  36% 97/270 [01:38<-1:55:29, -0.64it/s, loss=nan, v_num=ryed]Epoch 12:  36% 97/270 [01:38<-1:55:29, -0.64it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  35% 7/20 [00:04<00:07,  1.78it/s][A
Validation DataLoader 0:  35% 7/20 [00:04<00:07,  1.78it/s][AEpoch 12:  36% 98/270 [01:39<-1:55:25, -0.62it/s, loss=nan, v_num=ryed]Epoch 12:  36% 98/270 [01:39<-1:55:25, -0.62it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  40% 8/20 [00:05<00:07,  1.70it/s][A
Validation DataLoader 0:  40% 8/20 [00:05<00:07,  1.70it/s][AEpoch 12:  37% 99/270 [01:40<-1:55:20, -0.61it/s, loss=nan, v_num=ryed]Epoch 12:  37% 99/270 [01:40<-1:55:20, -0.61it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  45% 9/20 [00:06<00:08,  1.25it/s][A
Validation DataLoader 0:  45% 9/20 [00:06<00:08,  1.25it/s][AEpoch 12:  37% 100/270 [01:41<-1:55:13, -0.59it/s, loss=nan, v_num=ryed]Epoch 12:  37% 100/270 [01:41<-1:55:13, -0.59it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  50% 10/20 [00:06<00:07,  1.30it/s][A
Validation DataLoader 0:  50% 10/20 [00:06<00:07,  1.30it/s][AEpoch 12:  37% 101/270 [01:42<-1:55:08, -0.58it/s, loss=nan, v_num=ryed]Epoch 12:  37% 101/270 [01:42<-1:55:08, -0.58it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  55% 11/20 [00:07<00:06,  1.46it/s][A
Validation DataLoader 0:  55% 11/20 [00:07<00:06,  1.46it/s][AEpoch 12:  38% 102/270 [01:42<-1:55:04, -0.57it/s, loss=nan, v_num=ryed]Epoch 12:  38% 102/270 [01:42<-1:55:04, -0.57it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  60% 12/20 [00:08<00:05,  1.36it/s][A
Validation DataLoader 0:  60% 12/20 [00:08<00:05,  1.36it/s][AEpoch 12:  38% 103/270 [01:43<-1:54:58, -0.55it/s, loss=nan, v_num=ryed]Epoch 12:  38% 103/270 [01:43<-1:54:58, -0.55it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  65% 13/20 [00:09<00:05,  1.32it/s][A
Validation DataLoader 0:  65% 13/20 [00:09<00:05,  1.32it/s][AEpoch 12:  39% 104/270 [01:44<-1:54:52, -0.54it/s, loss=nan, v_num=ryed]Epoch 12:  39% 104/270 [01:44<-1:54:52, -0.54it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  70% 14/20 [00:09<00:04,  1.36it/s][A
Validation DataLoader 0:  70% 14/20 [00:09<00:04,  1.36it/s][AEpoch 12:  39% 105/270 [01:44<-1:54:46, -0.52it/s, loss=nan, v_num=ryed]Epoch 12:  39% 105/270 [01:44<-1:54:46, -0.52it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  75% 15/20 [00:10<00:03,  1.59it/s][A
Validation DataLoader 0:  75% 15/20 [00:10<00:03,  1.59it/s][AEpoch 12:  39% 106/270 [01:45<-1:54:41, -0.51it/s, loss=nan, v_num=ryed]Epoch 12:  39% 106/270 [01:45<-1:54:41, -0.51it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  80% 16/20 [00:10<00:02,  1.69it/s][A
Validation DataLoader 0:  80% 16/20 [00:10<00:02,  1.69it/s][AEpoch 12:  40% 107/270 [01:45<-1:54:35, -0.50it/s, loss=nan, v_num=ryed]Epoch 12:  40% 107/270 [01:45<-1:54:35, -0.50it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  85% 17/20 [00:11<00:01,  1.65it/s][A
Validation DataLoader 0:  85% 17/20 [00:11<00:01,  1.65it/s][AEpoch 12:  40% 108/270 [01:46<-1:54:29, -0.49it/s, loss=nan, v_num=ryed]Epoch 12:  40% 108/270 [01:46<-1:54:29, -0.49it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  90% 18/20 [00:12<00:01,  1.55it/s][A
Validation DataLoader 0:  90% 18/20 [00:12<00:01,  1.55it/s][AEpoch 12:  40% 109/270 [01:47<-1:54:22, -0.48it/s, loss=nan, v_num=ryed]Epoch 12:  40% 109/270 [01:47<-1:54:22, -0.48it/s, loss=nan, v_num=ryed]
Validation DataLoader 0:  95% 19/20 [00:13<00:00,  1.30it/s][A
Validation DataLoader 0:  95% 19/20 [00:13<00:00,  1.30it/s][AEpoch 12:  41% 110/270 [01:48<-1:54:14, -0.46it/s, loss=nan, v_num=ryed]Epoch 12:  41% 110/270 [01:48<-1:54:14, -0.46it/s, loss=nan, v_num=ryed]
Validation DataLoader 0: 100% 20/20 [00:13<00:00,  1.55it/s][A
Validation DataLoader 0: 100% 20/20 [00:13<00:00,  1.55it/s][AEpoch 12:  41% 111/270 [01:48<-1:54:08, -0.45it/s, loss=nan, v_num=ryed]Epoch 12:  41% 111/270 [01:48<-1:54:08, -0.45it/s, loss=nan, v_num=ryed]Epoch 12:  41% 111/270 [01:48<-1:54:08, -0.45it/s, loss=nan, v_num=ryed]
                                                            [AEpoch 12:  41% 111/270 [01:48<-1:54:08, -0.45it/s, loss=nan, v_num=ryed]/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "train_gnn.py", line 111, in <module>
    main()
  File "train_gnn.py", line 106, in main
    trainer.fit(model, ckpt_path=args.checkpoint)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 205, in run
    self.on_advance_end()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 297, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1636, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 308, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 377, in _save_topk_checkpoint
    raise MisconfigurationException(m)
pytorch_lightning.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='tot_auc')` could not find the monitored key in the returned metrics: ['train_loss', 'epoch', 'step']. HINT: Did you call `log('tot_auc', value)` in the `LightningModule`?
Traceback (most recent call last):
  File "train_gnn.py", line 111, in <module>
    main()
  File "train_gnn.py", line 106, in main
    trainer.fit(model, ckpt_path=args.checkpoint)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 205, in run
    self.on_advance_end()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 297, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1636, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 308, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 377, in _save_topk_checkpoint
    raise MisconfigurationException(m)
pytorch_lightning.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='tot_auc')` could not find the monitored key in the returned metrics: ['train_loss', 'epoch', 'step']. HINT: Did you call `log('tot_auc', value)` in the `LightningModule`?
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)Traceback (most recent call last):
  File "train_gnn.py", line 111, in <module>
    main()
  File "train_gnn.py", line 106, in main
    trainer.fit(model, ckpt_path=args.checkpoint)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 205, in run
    self.on_advance_end()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 297, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1636, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 308, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 377, in _save_topk_checkpoint
    raise MisconfigurationException(m)
pytorch_lightning.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='tot_auc')` could not find the monitored key in the returned metrics: ['train_loss', 'epoch', 'step']. HINT: Did you call `log('tot_auc', value)` in the `LightningModule`?
wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.060 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.060 MB uploaded (0.000 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.000 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.000 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.000 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.000 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.000 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.000 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced wobbly-plant-11: https://wandb.ai/pmtuan/ITk_Toy_HeteroGNN/runs/2gu1ryed
wandb: Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /global/cfs/cdirs/m3443/data/lightning_models/lightning_checkpoints/wandb/run-20220928_101622-2gu1ryed/logs
Traceback (most recent call last):
  File "train_gnn.py", line 111, in <module>
    main()
  File "train_gnn.py", line 106, in main
    trainer.fit(model, ckpt_path=args.checkpoint)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 205, in run
    self.on_advance_end()
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 297, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1636, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 308, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/global/cfs/cdirs/m3443/usr/pmtuan/conda/exatrkx_hsf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 377, in _save_topk_checkpoint
    raise MisconfigurationException(m)
pytorch_lightning.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='tot_auc')` could not find the monitored key in the returned metrics: ['train_loss', 'epoch', 'step']. HINT: Did you call `log('tot_auc', value)` in the `LightningModule`?
srun: error: nid002132: task 1: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=3280036.0
slurmstepd: error: *** STEP 3280036.0 ON nid002132 CANCELLED AT 2022-09-28T17:18:35 ***
srun: error: nid002132: tasks 0,2: Exited with exit code 1
srun: error: nid002132: task 3: Exited with exit code 1
