{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/cfs/cdirs/m3443/usr/pmtuan/conda/extrkx_hsf_clone/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import yaml, os\n",
    "import matplotlib, seaborn as sns\n",
    "from torch_geometric.data import HeteroData, Dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "from itertools import combinations_with_replacement, product\n",
    "from typing import Dict, Optional\n",
    "from torch import Tensor\n",
    "from torch_geometric.typing import Adj, EdgeType, NodeType\n",
    "import pdb\n",
    "import numpy as np\n",
    "import sys \n",
    "sys.path.append('/global/cfs/cdirs/m3443/usr/pmtuan/Tracking-ML-Exa.TrkX')\n",
    "%matplotlib inline\n",
    "\n",
    "CONFIGFILE = \"hetero_reg3_lev3.yaml\"\n",
    "with open(CONFIGFILE, 'r') as f: config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# data_dir = config['input_dir'] + '/train'\n",
    "# data = torch.load(os.path.join(data_dir, os.listdir(data_dir)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_dataset, LargeDataset, background_cut_event, make_mlp, process_data, convert_triplet_graph\n",
    "import torch_geometric.transforms as T\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_region(model):\n",
    "    return 'volume_' + '_'.join([str(i) for i in model['volume_ids']])\n",
    "\n",
    "def process_data(events, pt_background_cut, pt_signal_cut, noise, triplets, input_cut):\n",
    "    # Handle event in batched form\n",
    "    if type(events) is not list:\n",
    "        events = [events]\n",
    "\n",
    "    # NOTE: Cutting background by pT BY DEFINITION removes noise\n",
    "    if pt_background_cut > 0 or not noise:\n",
    "        for i, event in enumerate(events):\n",
    "\n",
    "            if triplets:  # Keep all event data for posterity!\n",
    "                event = convert_triplet_graph(event)\n",
    "\n",
    "            else:\n",
    "                event = background_cut_event(event, pt_background_cut, pt_signal_cut)                \n",
    "                    \n",
    "    for i, event in enumerate(events):\n",
    "        \n",
    "        # Ensure PID definition is correct\n",
    "        event.y_pid = (event.pid[event.edge_index[0]] == event.pid[event.edge_index[1]]) & event.pid[event.edge_index[0]].bool()\n",
    "        event.pid_signal = torch.isin(event.edge_index, event.signal_true_edges).all(0) & event.y_pid\n",
    "        \n",
    "        if (input_cut is not None) and \"scores\" in event.keys:\n",
    "            score_mask = event.scores > input_cut\n",
    "            for edge_attr in [\"edge_index\", \"y\", \"y_pid\", \"pid_signal\", \"scores\"]:\n",
    "                event[edge_attr] = event[edge_attr][..., score_mask]            \n",
    "\n",
    "    return events[0]\n",
    "\n",
    "# class LargeHeteroDataset(Dataset):\n",
    "#     def __init__(self, root, subdir, hparams, num_events=-1, transform=None, pre_transform=None, pre_filter=None):\n",
    "#         super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        \n",
    "#         self.subdir = subdir\n",
    "#         self.hparams = hparams\n",
    "#         # if transform is not None:\n",
    "#         #     from functools import partial\n",
    "#         #     self.transform = partial(transform, pt_background_cut=self.hparams['pt_background_cut'], pt_signal_cut=self.hparams['pt_signal_cut'], noise=self.hparams['noise'], triplets=False, input_cut=None)\n",
    "        \n",
    "#         self.input_paths = os.listdir(os.path.join(root, subdir))\n",
    "#         if \"sorted_events\" in hparams.keys() and hparams[\"sorted_events\"]:\n",
    "#             self.input_paths = sorted(self.input_paths)\n",
    "#         else:\n",
    "#             random.shuffle(self.input_paths)\n",
    "        \n",
    "#         self.input_paths = [os.path.join(root, subdir, event) for event in self.input_paths][:num_events]\n",
    "        \n",
    "#     def len(self):\n",
    "#         return len(self.input_paths)\n",
    "    \n",
    "#     def get(self, idx):\n",
    "#         event = torch.load(self.input_paths[idx], map_location=torch.device(\"cpu\"))     \n",
    "\n",
    "#         map = torch.zeros_like(event.hid)\n",
    "#         for model in self.hparams['model_ids']:\n",
    "#             volume_id = model['volume_ids']\n",
    "#             homo_ids = event.hid[ torch.isin( event.volume_id, torch.tensor(volume_id) ) ]\n",
    "#             map[homo_ids] = torch.arange(homo_ids.shape[0])\n",
    "        \n",
    "\n",
    "#         data = HeteroData()\n",
    "#         for model in self.hparams['model_ids']:\n",
    "#             region = get_region(model)\n",
    "#             mask = torch.isin( event.volume_id, torch.tensor(model['volume_ids']) )\n",
    "#             for attr in ['x', 'cell_data', 'pid', 'hid', 'pt', 'primary', 'nhits', 'modules', 'volume_id']:\n",
    "#                 data[region][attr] = event[attr][mask]\n",
    "#             data[region]['mask'] = mask\n",
    "        \n",
    "#         for model1, model2 in product(self.hparams['model_ids'], self.hparams['model_ids']):\n",
    "#             # ids = torch.tensor([model1['volume_ids'], model2['volume_ids']])\n",
    "#             id0, id1 = torch.tensor([model1['volume_ids']]), torch.tensor([model2['volume_ids']])\n",
    "#             region1, region2 = get_region(model1), get_region(model2)\n",
    "#             mask0 = torch.isin(event.volume_id[event.edge_index[0]], id0)\n",
    "#             mask1 = torch.isin(event.volume_id[event.edge_index[1]], id1)\n",
    "#             mask = mask1 * mask0 #+ torch.isin(event.volume_id[event.edge_index[0]], id2) * torch.isin(event.volume_id[event.edge_index[1]],id1)\n",
    "#             edge_index = event.edge_index.T[mask].T\n",
    "#             edge_index = map[edge_index]\n",
    "#             data[region1, 'connected_to', region2].edge_index = edge_index\n",
    "#             data[region1, 'connected_to', region2].y = event.y[mask]\n",
    "#             data[region1, 'connected_to', region2].y_pid = event.y_pid[mask]\n",
    "#             for truth_edge in ['modulewise_true_edges', 'signal_true_edges']:\n",
    "#                 mask = torch.isin(event.volume_id[event[truth_edge][0]], id0) * torch.isin(event.volume_id[event[truth_edge][1]], id1) #+ torch.isin(event.volume_id[event[truth_edge][0]], id2) * torch.isin(event.volume_id[event[truth_edge][1]], id1)\n",
    "#                 data[region1, 'connected_to', region2][truth_edge] = event[truth_edge].T[mask].T\n",
    "#         return data \n",
    "\n",
    "# class LargeHeteroDataset(LargeDataset):\n",
    "#     def __init__(self, root, subdir, hparams, num_events=-1, transform=None, pre_transform=None, pre_filter=None):\n",
    "#         super().__init__(root, subdir, hparams, num_events, transform, pre_transform, pre_filter)\n",
    "#         print(self.__getitem__)\n",
    "\n",
    "#     # def get_region(self, model):\n",
    "#     #     return 'region_' + '_'.join([str(i) for i in model['volume_ids']])\n",
    "\n",
    "#     def get(self, idx):\n",
    "        \n",
    "#         event = self.__getitem__(idx)\n",
    "\n",
    "#         # Process event with pt cuts\n",
    "#         # if self.hparams[\"pt_background_cut\"] > 0:\n",
    "#         #     event = background_cut_event(event, self.hparams[\"pt_background_cut\"], self.hparams[\"pt_signal_cut\"])\n",
    "        \n",
    "#         # Ensure PID definition is correct\n",
    "#         # event.y_pid = (event.pid[event.edge_index[0]] == event.pid[event.edge_index[1]]) & event.pid[event.edge_index[0]].bool()\n",
    "#         # event.pid_signal = torch.isin(event.edge_index, event.signal_true_edges).all(0) & event.y_pid\n",
    "\n",
    "#         # create new hit map\n",
    "#         models = config['model_ids']\n",
    "#         map = torch.zeros_like(event.hid)\n",
    "#         for model in self.hparams['model_ids']:\n",
    "#             volume_id = model['volume_ids']\n",
    "#             homo_ids = event.hid[ torch.isin( event.volume_id, torch.tensor(volume_id) ) ]\n",
    "#             map[homo_ids] = torch.arange(homo_ids.shape[0])\n",
    "        \n",
    "\n",
    "#         data = HeteroData()\n",
    "#         for model in self.hparams['model_ids']:\n",
    "#             region = get_region(model)\n",
    "#             mask = torch.isin( event.volume_id, torch.tensor(model['volume_ids']) )\n",
    "#             for attr in ['x', 'cell_data', 'pid', 'hid', 'pt', 'primary', 'nhits', 'modules', 'volume_id']:\n",
    "#                 data[region][attr] = event[attr][mask]\n",
    "#             data[region]['mask'] = mask\n",
    "        \n",
    "#         for model1, model2 in product(self.hparams['model_ids'], self.hparams['model_ids']):\n",
    "#             # ids = torch.tensor([model1['volume_ids'], model2['volume_ids']])\n",
    "#             id0, id1 = torch.tensor([model1['volume_ids']]), torch.tensor([model2['volume_ids']])\n",
    "#             region1, region2 = get_region(model1), get_region(model2)\n",
    "#             mask0 = torch.isin(event.volume_id[event.edge_index[0]], id0)\n",
    "#             mask1 = torch.isin(event.volume_id[event.edge_index[1]], id1)\n",
    "#             mask = mask1 * mask0 #+ torch.isin(event.volume_id[event.edge_index[0]], id2) * torch.isin(event.volume_id[event.edge_index[1]],id1)\n",
    "#             edge_index = event.edge_index.T[mask].T\n",
    "#             edge_index = map[edge_index]\n",
    "#             data[region1, 'connected_to', region2].edge_index = edge_index\n",
    "#             data[region1, 'connected_to', region2].y = event.y[mask]\n",
    "#             data[region1, 'connected_to', region2].y_pid = event.y_pid[mask]\n",
    "#             for truth_edge in ['modulewise_true_edges', 'signal_true_edges']:\n",
    "#                 mask = torch.isin(event.volume_id[event[truth_edge][0]], id0) * torch.isin(event.volume_id[event[truth_edge][1]], id1) #+ torch.isin(event.volume_id[event[truth_edge][0]], id2) * torch.isin(event.volume_id[event[truth_edge][1]], id1)\n",
    "#                 data[region1, 'connected_to', region2][truth_edge] = event[truth_edge].T[mask].T\n",
    "#         return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[297481, 9], cell_data=[297481, 11], pid=[297481], event_file='/global/cfs/cdirs/m3443/data/ITk-upgrade/processed/full_events_v4/event000015937', hid=[297481], pt=[297481], primary=[297481], nhits=[297481], modules=[297481], modulewise_true_edges=[2, 116389], signal_true_edges=[2, 12588], edge_index=[2, 574596], y=[574596], y_pid=[574596], volume_id=[297481])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(os.path.join(config['input_dir'], 'train'))\n",
    "e = torch.load(os.path.join(config['input_dir'], 'train', files[0]))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pipelines.Common_Tracking_Example.LightningModules.GNN.hetero_dataset import LargeHeteroDataset\n",
    "transform = partial(process_data, pt_background_cut=config['pt_background_cut'], pt_signal_cut=config['pt_signal_cut'], noise=config['noise'], triplets=False, input_cut=None)\n",
    "dataset = LargeHeteroDataset(root=config['input_dir'], subdir='train', num_events=10, hparams=config, process_function=transform)\n",
    "data = dataset.get(0)\n",
    "# homo_data = torch.load(input_dir)\n",
    "# undirected_data = T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, HeteroLinear, MLP, GCNConv, MessagePassing, to_hetero\n",
    "from torch.nn import Module, ModuleDict\n",
    "from collections import defaultdict\n",
    "\n",
    "class NodeEncoder(torch.nn.Module):\n",
    "    def __init__(self, hparams, model) -> None:\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.network = make_mlp(\n",
    "            model['num_features'],\n",
    "            [hparams[\"hidden\"]] * hparams[\"nb_node_layer\"],\n",
    "            output_activation=hparams[\"output_activation\"],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return  self.network( x.float() )\n",
    "    \n",
    "class HeteroNodeEncoder(torch.nn.Module):\n",
    "    def __init__(self, hparams) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.encoders = torch.nn.ModuleDict()\n",
    "        for model in self.hparams['model_ids']:\n",
    "            region = get_region(model)\n",
    "            self.encoders[region] = NodeEncoder(self.hparams, model)\n",
    "        \n",
    "    def forward(self, x_dict):\n",
    "        for model in self.hparams['model_ids']:\n",
    "            region = get_region(model)\n",
    "            print(region)\n",
    "            x_dict[region] = self.encoders[region](x_dict[region][:, : model['num_features']])\n",
    "        \n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'volume_0_1': tensor([[ 0.4559, -0.6419, -0.2931,  ...,  0.0601,  0.2524,  0.7009],\n",
       "         [ 0.4751, -0.6559, -0.3046,  ...,  0.0529,  0.2268,  0.7113],\n",
       "         [ 0.4322, -0.6406, -0.2763,  ...,  0.0861,  0.2159,  0.7096],\n",
       "         ...,\n",
       "         [-0.9515, -0.9330, -0.9221,  ..., -0.2887,  0.8334, -0.8976],\n",
       "         [-0.9532, -0.9333, -0.9220,  ..., -0.2904,  0.8344, -0.8974],\n",
       "         [-0.9538, -0.9331, -0.9231,  ..., -0.2832,  0.8338, -0.8953]],\n",
       "        grad_fn=<TanhBackward0>),\n",
       " 'volume_2': tensor([[ 0.5418,  0.9371,  0.8508,  ..., -0.9120,  0.1569,  0.0486],\n",
       "         [ 0.5762,  0.9358,  0.8506,  ..., -0.9064,  0.1539,  0.0734],\n",
       "         [ 0.4515,  0.9366,  0.8540,  ..., -0.9223,  0.1309, -0.0043],\n",
       "         ...,\n",
       "         [ 0.3760, -0.8509, -0.4800,  ..., -0.7841,  0.7006, -0.4799],\n",
       "         [ 0.3826, -0.8437, -0.4772,  ..., -0.7881,  0.7012, -0.4746],\n",
       "         [ 0.4670, -0.7989, -0.5056,  ..., -0.8094,  0.7284, -0.4121]],\n",
       "        grad_fn=<TanhBackward0>),\n",
       " 'volume_3': tensor([[ 0.9449, -0.5705, -0.1779,  ..., -0.8334, -0.7380, -0.6303],\n",
       "         [ 0.9434, -0.5818, -0.1685,  ..., -0.8169, -0.7216, -0.6515],\n",
       "         [-0.9647, -0.7614, -0.7734,  ...,  0.6438,  0.8145,  0.9178],\n",
       "         ...,\n",
       "         [-0.6866,  0.5762,  0.2488,  ...,  0.1966,  0.4573, -0.2291],\n",
       "         [-0.6867,  0.5768,  0.2484,  ...,  0.1968,  0.4576, -0.2287],\n",
       "         [-0.6844,  0.5716,  0.2501,  ...,  0.2090,  0.4463, -0.2421]],\n",
       "        grad_fn=<TanhBackward0>)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = HeteroNodeEncoder(hparams=config)\n",
    "\n",
    "encoder(data.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeEncoder(torch.nn.Module):\n",
    "    def __init__(self, hparams) -> None:\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.network = make_mlp(\n",
    "            2 * hparams['hidden'],\n",
    "            [hparams[\"hidden\"]] * hparams[\"nb_node_layer\"],\n",
    "            output_activation=hparams[\"output_activation\"],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        src, dst = edge_index\n",
    "        print(\"Encoding edges\")\n",
    "        if isinstance(x, tuple):\n",
    "            x1, x2 = x\n",
    "            x_in = torch.cat([x1[src], x2[dst]], dim=-1)\n",
    "        else:\n",
    "            x_in = torch.cat([x[src], x[dst]], dim=-1)\n",
    "\n",
    "        return  self.network( x_in )\n",
    "\n",
    "class HeteroEdgeConv(HeteroConv):\n",
    "\n",
    "    def __init__(self, convs: dict, aggr: str = \"sum\"):\n",
    "        super().__init__(convs, aggr)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_dict: dict,\n",
    "        edge_index_dict: dict,\n",
    "        *args_dict,\n",
    "        **kwargs_dict,\n",
    "    ) -> dict :\n",
    "\n",
    "        out_dict = {}\n",
    "        for edge_type, edge_index in edge_index_dict.items():\n",
    "            print(edge_type)\n",
    "            src, rel, dst = edge_type\n",
    "\n",
    "            str_edge_type = '__'.join(edge_type)\n",
    "            if str_edge_type not in self.convs:\n",
    "                continue\n",
    "\n",
    "            args = []\n",
    "            for value_dict in args_dict:\n",
    "                if edge_type in value_dict:\n",
    "                    args.append(value_dict[edge_type])\n",
    "                elif src == dst and src in value_dict:\n",
    "                    args.append(value_dict[src])\n",
    "                elif src in value_dict or dst in value_dict:\n",
    "                    args.append(\n",
    "                        (value_dict.get(src, None), value_dict.get(dst, None)))\n",
    "\n",
    "            kwargs = {}\n",
    "            for arg, value_dict in kwargs_dict.items():\n",
    "                arg = arg[:-5]  # `{*}_dict`\n",
    "                if edge_type in value_dict:\n",
    "                    kwargs[arg] = value_dict[edge_type]\n",
    "                elif src == dst and src in value_dict:\n",
    "                    kwargs[arg] = value_dict[src]\n",
    "                elif src in value_dict or dst in value_dict:\n",
    "                    kwargs[arg] = (value_dict.get(src, None),\n",
    "                                value_dict.get(dst, None))\n",
    "\n",
    "            conv = self.convs[str_edge_type]\n",
    "            print(kwargs)\n",
    "\n",
    "            if src == dst:\n",
    "                out = conv(x_dict[src], edge_index, *args, **kwargs)\n",
    "            else:\n",
    "                out = conv((x_dict[src], x_dict[dst]), edge_index, *args, **kwargs)\n",
    "\n",
    "            out_dict[edge_type] = out\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "class EdgeClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = make_mlp(\n",
    "            3 * hparams[\"hidden\"],\n",
    "            [hparams[\"hidden\"]] * hparams[\"nb_edge_layer\"] + [1],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "            output_activation=hparams['output_activation'],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge):\n",
    "        src, dst = edge_index\n",
    "        if isinstance(x, tuple):\n",
    "            x1, x2 = x\n",
    "            classifier_input = torch.cat([x1[src], x2[dst], edge], dim=-1)\n",
    "        else:\n",
    "            classifier_input = torch.cat([x[src], x[dst], edge], dim=-1)\n",
    "        # classifier_input = torch.cat([x[src], x[dst], edge], dim=-1)\n",
    "        return self.network(classifier_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionMessagePassing(MessagePassing):\n",
    "    def __init__(self, hparams, aggr: str = \"add\", flow: str = \"source_to_target\", node_dim: int = -2, decomposed_layers: int = 1):\n",
    "        super().__init__(aggr, flow=flow, node_dim=node_dim, decomposed_layers=decomposed_layers)\n",
    "\n",
    "        self.hparams=hparams\n",
    "\n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_encoder = make_mlp(\n",
    "            2 * (hparams[\"hidden\"]),\n",
    "            [hparams[\"hidden\"]] * hparams[\"nb_edge_layer\"],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "            output_activation=hparams[\"output_activation\"],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "        )\n",
    "\n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_network = make_mlp(\n",
    "            3 * hparams[\"hidden\"],\n",
    "            [hparams[\"hidden\"]] * hparams[\"nb_edge_layer\"],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "            output_activation=hparams[\"output_activation\"],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "        )\n",
    "\n",
    "        # The node network computes new node features\n",
    "        self.node_network = make_mlp(\n",
    "            2 * hparams[\"hidden\"],\n",
    "            [hparams[\"hidden\"]] * hparams[\"nb_node_layer\"],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "            output_activation=hparams[\"output_activation\"],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "        )\n",
    "\n",
    "    def message(self, edge):\n",
    "        return edge\n",
    "\n",
    "    def aggregate(self, out, edge_index):\n",
    "\n",
    "        src, dst = edge_index\n",
    "        return self.aggr_module(out, dst)[dst.unique()]\n",
    "    \n",
    "    def update(self, agg_message, x, edge_index):\n",
    "        src, dst = edge_index\n",
    "        indices_to_add = torch.arange(agg_message.shape[0])\n",
    "        print(dst.unique())\n",
    "        print(x)\n",
    "        x[dst.unique()] += agg_message\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def edge_update(self, x, edge, edge_index, *args, **kwargs):\n",
    "        src, dst = edge_index\n",
    "        if isinstance(x, tuple):\n",
    "            x_src, x_dst = x[0][src], x[1][dst]\n",
    "        else:\n",
    "            x_src, x_dst = x[src], x[dst]\n",
    "        out = self.edge_network(torch.cat([x_src, x_dst, edge], dim=-1))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, edge_index, edge):\n",
    "\n",
    "        if isinstance(x, tuple):\n",
    "            x_src, x_dst = x\n",
    "        else:\n",
    "            x_src, x_dst = x, x\n",
    "\n",
    "        x_dst = self.propagate(edge_index, x=x_dst, edge=edge)\n",
    "\n",
    "        return x_dst\n",
    "\n",
    "class InteractionHeteroConv(HeteroConv):\n",
    "    def __init__(self, convs: Dict[EdgeType, Module], aggr: Optional[str] = \"sum\"):\n",
    "        super().__init__(convs, aggr)\n",
    "\n",
    "    def edge_forward(self,x_dict: Dict[NodeType, Tensor],\n",
    "        edge_index_dict: Dict[EdgeType, Adj],\n",
    "        edge_dict,\n",
    "        *args_dict,\n",
    "        **kwargs_dict,\n",
    "    ) -> Dict[NodeType, Tensor]:\n",
    "\n",
    "        out_dict = {}\n",
    "        for edge_type, edge_index in edge_index_dict.items():\n",
    "            print(edge_type)\n",
    "            src, rel, dst = edge_type\n",
    "\n",
    "            str_edge_type = '__'.join(edge_type)\n",
    "            if str_edge_type not in self.convs:\n",
    "                continue\n",
    "\n",
    "            args = []\n",
    "            for value_dict in args_dict:\n",
    "                if edge_type in value_dict:\n",
    "                    args.append(value_dict[edge_type])\n",
    "                elif src == dst and src in value_dict:\n",
    "                    args.append(value_dict[src])\n",
    "                elif src in value_dict or dst in value_dict:\n",
    "                    args.append(\n",
    "                        (value_dict.get(src, None), value_dict.get(dst, None)))\n",
    "\n",
    "            kwargs = {}\n",
    "            for arg, value_dict in kwargs_dict.items():\n",
    "                arg = arg[:-5]  # `{*}_dict`\n",
    "                if edge_type in value_dict:\n",
    "                    kwargs[arg] = value_dict[edge_type]\n",
    "                elif src == dst and src in value_dict:\n",
    "                    kwargs[arg] = value_dict[src]\n",
    "                elif src in value_dict or dst in value_dict:\n",
    "                    kwargs[arg] = (value_dict.get(src, None),\n",
    "                                value_dict.get(dst, None))\n",
    "\n",
    "            conv = self.convs[str_edge_type]\n",
    "            edge = edge_dict[edge_type]\n",
    "\n",
    "            out = conv.edge_update((x_dict[src], x_dict[dst]), edge, edge_index, *args, **kwargs)\n",
    "\n",
    "            # if src == dst:\n",
    "            #     out = conv.edge_updater(x_dict[src], edge_index, *args, **kwargs)\n",
    "            # else:\n",
    "            #     out = conv.edge_update((x_dict[src], x_dict[dst]), edge_index, *args,\n",
    "            #             **kwargs)\n",
    "\n",
    "            out_dict[edge_type] = out\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.node_encoders = HeteroNodeEncoder(self.hparams)\n",
    "        self.edge_encoders = HeteroEdgeConv({\n",
    "            (get_region(model0), 'connected_to', get_region(model1)): EdgeEncoder(self.hparams)\n",
    "            for model0, model1 in product(self.hparams['model_ids'], self.hparams['model_ids'])\n",
    "        })\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for _ in range(2):\n",
    "            conv = InteractionHeteroConv({\n",
    "                (get_region(model0), 'connected_to', get_region(model1)): InteractionMessagePassing(hparams=self.hparams)\n",
    "                for model0, model1 in product(self.hparams['model_ids'], self.hparams['model_ids'])\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.edge_classifiers = HeteroEdgeConv({\n",
    "            (get_region(model0), 'connected_to', get_region(model1)): EdgeClassifier(self.hparams)\n",
    "            for model0, model1 in combinations_with_replacement(self.hparams['model_ids'], 2)\n",
    "        })\n",
    "\n",
    "\n",
    "    def forward(self, x_dict: dict, edge_index_dict: dict):\n",
    "        x_dict = self.node_encoders(x_dict)  \n",
    "        edge_dict = self.edge_encoders(x_dict, edge_index_dict) \n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict, edge_dict=edge_dict)\n",
    "            edge_dict = conv.edge_forward(x_dict, edge_index_dict, edge_dict)\n",
    "\n",
    "        out = self.edge_classifiers(x_dict, edge_index_dict, edge_dict=edge_dict)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import torch.functional as F\n",
    "\n",
    "class HeteroGNNBase(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Initialise the Lightning Module that can scan over different GNN training regimes\n",
    "        \"\"\"\n",
    "\n",
    "        # Assign hyperparameters\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.trainset, self.valset, self.testset = None, None, None\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Handle any subset of [train, val, test] data split, assuming that ordering\n",
    "\n",
    "        if self.trainset is None:\n",
    "            print(\"Setting up dataset\")\n",
    "            input_subdirs = [None, None, None]\n",
    "            input_subdirs[: len(self.hparams[\"datatype_names\"])] = [\n",
    "                os.path.join(self.hparams[\"input_dir\"], datatype)\n",
    "                for datatype in self.hparams[\"datatype_names\"]\n",
    "            ]\n",
    "            self.trainset, self.valset, self.testset = [\n",
    "                load_dataset(\n",
    "                    input_subdir=input_subdir,\n",
    "                    num_events=self.hparams[\"datatype_split\"][i],\n",
    "                    **self.hparams\n",
    "                )\n",
    "                for i, input_subdir in enumerate(input_subdirs)\n",
    "            ]\n",
    "\n",
    "        if (\n",
    "            (self.trainer)\n",
    "            and (\"logger\" in self.trainer.__dict__.keys())\n",
    "            and (\"_experiment\" in self.logger.__dict__.keys())\n",
    "        ):\n",
    "            self.logger.experiment.define_metric(\"val_loss\", summary=\"min\")\n",
    "            self.logger.experiment.define_metric(\"sig_auc\", summary=\"max\")\n",
    "            self.logger.experiment.define_metric(\"tot_auc\", summary=\"max\")\n",
    "            self.logger.experiment.define_metric(\"sig_fake_ratio\", summary=\"max\")\n",
    "            self.logger.experiment.define_metric(\"custom_f1\", summary=\"max\")\n",
    "            self.logger.experiment.log({\"sig_auc\": 0})\n",
    "            self.logger.experiment.log({\"sig_fake_ratio\": 0})\n",
    "            self.logger.experiment.log({\"custom_f1\": 0})\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.trainset is not None:\n",
    "            return DataLoader(\n",
    "                self.trainset, batch_size=1, num_workers=1\n",
    "            )  # , pin_memory=True, persistent_workers=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.valset is not None:\n",
    "            return DataLoader(\n",
    "                self.valset, batch_size=1, num_workers=1\n",
    "            )  # , pin_memory=True, persistent_workers=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.testset is not None:\n",
    "            return DataLoader(\n",
    "                self.testset, batch_size=1, num_workers=1\n",
    "            )  # , pin_memory=True, persistent_workers=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = [\n",
    "            torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=(self.hparams[\"lr\"]),\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-08,\n",
    "                amsgrad=True,\n",
    "            )\n",
    "        ]\n",
    "        scheduler = [\n",
    "            {\n",
    "                \"scheduler\": torch.optim.lr_scheduler.StepLR(\n",
    "                    optimizer[0],\n",
    "                    step_size=self.hparams[\"patience\"],\n",
    "                    gamma=self.hparams[\"factor\"],\n",
    "                ),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        ]\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def handle_directed(self, batch, edge_sample, truth_sample, sample_indices):\n",
    "\n",
    "        edge_sample = torch.cat([edge_sample, edge_sample.flip(0)], dim=-1)\n",
    "        truth_sample = truth_sample.repeat(2)\n",
    "        sample_indices = sample_indices.repeat(2)\n",
    "\n",
    "        if (\"directed\" in self.hparams.keys()) and self.hparams[\"directed\"]:\n",
    "            direction_mask = batch.x[edge_sample[0], 0] < batch.x[edge_sample[1], 0]\n",
    "            edge_sample = edge_sample[:, direction_mask]\n",
    "            truth_sample = truth_sample[direction_mask]\n",
    "\n",
    "        return edge_sample, truth_sample, sample_indices\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # truth = batch[self.hparams[\"truth_key\"]]\n",
    "\n",
    "        # currently, we don't train purity\n",
    "        # if (\"train_purity\" in self.hparams.keys()) and (\n",
    "        #     self.hparams[\"train_purity\"] > 0\n",
    "        # ):\n",
    "        #     edge_sample, truth_sample, sample_indices = purity_sample(\n",
    "        #         truth, batch.edge_index, self.hparams[\"train_purity\"]\n",
    "        #     )\n",
    "        # else:\n",
    "        #     edge_sample, truth_sample, sample_indices = batch.edge_index, truth, torch.arange(batch.edge_index.shape[1])\n",
    "            \n",
    "        # edge_sample, truth_sample, sample_indices = self.handle_directed(batch, edge_sample, truth_sample, sample_indices)\n",
    "\n",
    "        # weight = (\n",
    "        #     torch.tensor(self.hparams[\"weight\"])\n",
    "        #     if (\"weight\" in self.hparams)\n",
    "        #     else torch.tensor((~truth_sample.bool()).sum() / truth_sample.sum())\n",
    "        # )\n",
    "\n",
    "        output_dict = self(batch.x_dict, batch.edge_index_dict)\n",
    "        truth_dict = batch.truth_dict\n",
    "        if self.hparams[\"mask_background\"]:\n",
    "            for key, output in output_dict.items():\n",
    "                y_subset = batch.truth_dict[key] | ~batch.y_pid_dict[key].bool() # previously the y_pid is filtered by the sample_indices\n",
    "                output_dict[key], truth_dict[key] = output_dict[key][y_subset], batch.truth_dict[key][y_subset]\n",
    "\n",
    "        loss = [\n",
    "            F.binary_cross_entropy_with_logits(\n",
    "                output, truth.float(), reduction='sum'\n",
    "            )\n",
    "            for output, truth in zip(output_dict.values(), truth_dict.values())\n",
    "        ]\n",
    "        loss /= np.sum([output.shape[0] for output in output_dict.values()])\n",
    "        # loss = F.binary_cross_entropy_with_logits(\n",
    "        #     output, truth_sample.float()#, pos_weight=weight\n",
    "        # )\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def log_metrics(self, output, sample_indices, batch, loss, log):\n",
    "\n",
    "        preds = torch.sigmoid(output) > self.hparams[\"edge_cut\"]\n",
    "\n",
    "        # Positives\n",
    "        edge_positive = preds.sum().float()\n",
    "\n",
    "        # Signal true & signal tp\n",
    "        sig_truth = batch.pid_signal[sample_indices]\n",
    "        sig_true = sig_truth.sum().float()\n",
    "        sig_true_positive = (sig_truth.bool() & preds).sum().float()\n",
    "        sig_auc = roc_auc_score(\n",
    "            sig_truth.bool().cpu().detach(), torch.sigmoid(output).cpu().detach()\n",
    "        )\n",
    "\n",
    "        # Total true & total tp\n",
    "        tot_truth = (batch.y_pid.bool() | batch.y.bool())[sample_indices]\n",
    "        tot_true = tot_truth.sum().float()\n",
    "        tot_true_positive = (tot_truth.bool() & preds).sum().float()\n",
    "        tot_auc = roc_auc_score(\n",
    "            tot_truth.bool().cpu().detach(), torch.sigmoid(output).cpu().detach()\n",
    "        )\n",
    "\n",
    "        # Eff, pur, auc\n",
    "        sig_eff = sig_true_positive / sig_true\n",
    "        sig_pur = sig_true_positive / edge_positive\n",
    "        tot_eff = tot_true_positive / tot_true\n",
    "        tot_pur = tot_true_positive / edge_positive\n",
    "\n",
    "        # Combined metrics\n",
    "        double_auc = sig_auc * tot_auc\n",
    "        custom_f1 = 2 * sig_eff * tot_pur / (sig_eff + tot_pur)\n",
    "        sig_fake_ratio = sig_true_positive / (edge_positive - tot_true_positive)\n",
    "\n",
    "        if log:\n",
    "            current_lr = self.optimizers().param_groups[0][\"lr\"]\n",
    "            self.log_dict(\n",
    "                {\n",
    "                    \"val_loss\": loss,\n",
    "                    \"current_lr\": current_lr,\n",
    "                    \"sig_eff\": sig_eff,\n",
    "                    \"sig_pur\": sig_pur,\n",
    "                    \"sig_auc\": sig_auc,\n",
    "                    \"tot_eff\": tot_eff,\n",
    "                    \"tot_pur\": tot_pur,\n",
    "                    \"tot_auc\": tot_auc,\n",
    "                    \"double_auc\": double_auc,\n",
    "                    \"custom_f1\": custom_f1,\n",
    "                    \"sig_fake_ratio\": sig_fake_ratio,\n",
    "                },\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def shared_evaluation(self, batch, batch_idx, log=True):\n",
    "\n",
    "        truth = batch[self.hparams[\"truth_key\"]]\n",
    "        \n",
    "        # if (\"train_purity\" in self.hparams.keys()) and (\n",
    "        #     self.hparams[\"train_purity\"] > 0\n",
    "        # ):\n",
    "        #     edge_sample, truth_sample, sample_indices = purity_sample(\n",
    "        #         truth, batch.edge_index, self.hparams[\"train_purity\"]\n",
    "        #     )\n",
    "        # else:\n",
    "        #     edge_sample, truth_sample, sample_indices = batch.edge_index, truth, torch.arange(batch.edge_index.shape[1])\n",
    "            \n",
    "        # edge_sample, truth_sample, sample_indices = self.handle_directed(batch, edge_sample, truth_sample, sample_indices)\n",
    "\n",
    "        # weight = (\n",
    "        #     torch.tensor(self.hparams[\"weight\"])\n",
    "        #     if (\"weight\" in self.hparams)\n",
    "        #     else torch.tensor((~truth_sample.bool()).sum() / truth_sample.sum())\n",
    "        # )\n",
    "        \n",
    "        # output = self(batch.x.float(), edge_sample, batch.volume_id).squeeze()\n",
    "\n",
    "        output_dict = self(batch.x_dict, batch.edge_index_dict)\n",
    "        truth_dict = batch.truth_dict\n",
    "        if self.hparams[\"mask_background\"]:\n",
    "            for key, output in output_dict.items():\n",
    "                y_subset = batch.truth_dict[key] | ~batch.y_pid_dict[key].bool() # previously the y_pid is filtered by the sample_indices\n",
    "                output_dict[key], truth_dict[key] = output_dict[key][y_subset], truth_dict[key][y_subset]\n",
    "\n",
    "        loss = [\n",
    "            F.binary_cross_entropy_with_logits(\n",
    "                output, truth.float(), reduction='sum'\n",
    "            )\n",
    "            for output, truth in zip(output_dict.values(), truth_dict.values())\n",
    "        ]\n",
    "        loss /= np.sum([output.shape[0] for output in output_dict.values()])\n",
    "\n",
    "        # if self.hparams[\"mask_background\"]:\n",
    "        #     y_subset = truth_sample | ~batch.y_pid[sample_indices].bool()\n",
    "        #     subset_output, subset_truth_sample = output[y_subset], truth_sample[y_subset]\n",
    "        #     loss = F.binary_cross_entropy_with_logits(\n",
    "        #         subset_output, subset_truth_sample.float().squeeze(), pos_weight=weight\n",
    "        #     )            \n",
    "        # else:\n",
    "        #     loss = F.binary_cross_entropy_with_logits(\n",
    "        #         output, truth_sample.float().squeeze(), pos_weight=weight\n",
    "        #     )\n",
    "\n",
    "        # try:\n",
    "        #     preds = self.log_metrics(output, sample_indices, batch, loss, log)\n",
    "        #     return {\"loss\": loss, \"preds\": preds, \"score\": torch.sigmoid(output)}\n",
    "        # except:\n",
    "        return {\"loss\": loss, \n",
    "                # \"score\": torch.sigmoid(output)\n",
    "            }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        outputs = self.shared_evaluation(batch, batch_idx)\n",
    "\n",
    "        return outputs[\"loss\"]\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        outputs = self.shared_evaluation(batch, batch_idx, log=False)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def test_step_end(self, output_results):\n",
    "\n",
    "        print(\"Step:\", output_results)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "\n",
    "        print(\"Epoch:\", outputs)\n",
    "\n",
    "    def optimizer_step(\n",
    "        self,\n",
    "        epoch,\n",
    "        batch_idx,\n",
    "        optimizer,\n",
    "        optimizer_idx,\n",
    "        optimizer_closure=None,\n",
    "        on_tpu=False,\n",
    "        using_native_amp=False,\n",
    "        using_lbfgs=False,\n",
    "    ):\n",
    "        # warm up lr\n",
    "        if (self.hparams[\"warmup\"] is not None) and (\n",
    "            self.current_epoch < self.hparams[\"warmup\"]\n",
    "        ):\n",
    "            lr_scale = min(\n",
    "                1.0, float(self.current_epoch + 1) / self.hparams[\"warmup\"]\n",
    "            )\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] = lr_scale * self.hparams[\"lr\"]\n",
    "\n",
    "        # update params\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "class LargeGNNBase(HeteroGNNBase):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "\n",
    "    def setup(self, stage, process_function):\n",
    "        # Handle any subset of [train, val, test] data split, assuming that ordering\n",
    "        splits = np.array(self.hparams['datatype_split']) \n",
    "        splits = (self.hparams['n_events'] * splits / np.sum(splits)).astype(np.int32)\n",
    "\n",
    "        self.trainset, self.valset, self.testset = [\n",
    "            LargeHeteroDataset(\n",
    "                self.hparams['input_dir'],\n",
    "                subdir,\n",
    "                self.hparams,\n",
    "                split,\n",
    "                process_function\n",
    "            )\n",
    "            for subdir, split in zip(self.hparams['datatype_names'], self.hparams['datatype_split'])\n",
    "        ]\n",
    "\n",
    "        if (\n",
    "            (self.trainer)\n",
    "            and (\"logger\" in self.trainer.__dict__.keys())\n",
    "            and (\"_experiment\" in self.logger.__dict__.keys())\n",
    "        ):\n",
    "            self.logger.experiment.define_metric(\"val_loss\", summary=\"min\")\n",
    "            self.logger.experiment.define_metric(\"sig_auc\", summary=\"max\")\n",
    "            self.logger.experiment.define_metric(\"tot_auc\", summary=\"max\")\n",
    "            self.logger.experiment.define_metric(\"sig_fake_ratio\", summary=\"max\")\n",
    "            self.logger.experiment.define_metric(\"custom_f1\", summary=\"max\")\n",
    "            self.logger.experiment.log({\"sig_auc\": 0})\n",
    "            self.logger.experiment.log({\"sig_fake_ratio\": 0})\n",
    "            self.logger.experiment.log({\"custom_f1\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('extrkx_hsf_clone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "771d61ecd5a3184c4db4d548640adff250f79a9f92b9bacc068bde90a3f9bd55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
